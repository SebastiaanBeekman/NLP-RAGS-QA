{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries by LLM; Are not installed per default\n",
    "# ! pip install accelerate\n",
    "# ! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, StoppingCriteriaList, StoppingCriteria,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Embeddings variables\n",
    "corpus_path = os.getenv(\"CORPUS_PATH\")\n",
    "\n",
    "# Retrieval variables\n",
    "queries_path = os.getenv(\"QUERIES_PATH\")\n",
    "\n",
    "## LLM variables\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "llm_id = os.getenv(\"LLM_ID\")\n",
    "num_docs = int(os.getenv(\"TOP_K\"))\n",
    "max_input_length = int(os.getenv(\"MAX_INPUT_LENGTH\"))\n",
    "max_output_length = int(os.getenv(\"MAX_OUTPUT_LENGTH\"))\n",
    "normalize_queries = os.getenv(\"NORMALIZE_QUERIES\") == \"True\"\n",
    "context_retrieval_dir = os.getenv(\"CONTEXT_RETRIEVAL_DIR\")\n",
    "llm_response_dir = os.getenv(\"LLM_RESPONSE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sebastiaanbeekman/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = pickle.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_pickle(data, file_path: str):\n",
    "    with open(file_path, \"wb\") as writer:\n",
    "        pickle.dump(data, writer)\n",
    "\n",
    "\n",
    "def read_json(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data\n",
    "\n",
    "def read_corpus(corpus_path: str):\n",
    "    new_corpus = []\n",
    "    corpus = read_json(corpus_path).values()\n",
    "    for i, record in enumerate(corpus):\n",
    "        record[\"full_corpus_idx\"] = i\n",
    "        new_corpus.append(record)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run seeder before proceeding\n",
    "set_seeds(seed)\n",
    "print(f\"Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    \"\"\"\n",
    "    A class for loading and generating text using a Language Model (LM) with support for quantization\n",
    "    and custom stopping criteria.\n",
    "    \n",
    "    Attributes:\n",
    "        model_id (str): Identifier for the model to load.\n",
    "        device (str): Device to run the model on, e.g. 'cuda'.\n",
    "        quantization_bits (Optional[int]): Number of bits for quantization, supports 4 or 8 bits.\n",
    "        stop_list (Optional[List[str]]): List of tokens where generation should stop.\n",
    "        model_max_length (int): Maximum length of the model inputs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str, \n",
    "        device: str = 'cuda', \n",
    "        quantization_bits: Optional[int] = None, \n",
    "        stop_list: Optional[List[str]] = None, \n",
    "        model_max_length: int = 4096\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        self.stop_list = stop_list\n",
    "        if stop_list is None:\n",
    "            self.stop_list = ['\\nHuman:', '\\n```\\n', '\\nQuestion:', '<|endoftext|>', '\\n']\n",
    "        \n",
    "        self.bnb_config = self._set_quantization(quantization_bits)\n",
    "        self.model, self.tokenizer = self._initialize_model_tokenizer(model_id)\n",
    "        self.stopping_criteria = self._define_stopping_criteria()\n",
    "        \n",
    "\n",
    "    def _set_quantization(self, quantization_bits: Optional[int]) -> Optional[BitsAndBytesConfig]:\n",
    "        \"\"\"\n",
    "        Configure quantization settings based on the specified number of bits.\n",
    "        \"\"\"\n",
    "        if quantization_bits in [4, 8]:\n",
    "            bnb_config = BitsAndBytesConfig()\n",
    "            if quantization_bits == 4:\n",
    "                bnb_config.load_in_4bit = True\n",
    "                bnb_config.bnb_4bit_quant_type = 'nf4'\n",
    "                bnb_config.bnb_4bit_use_double_quant = True\n",
    "                bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            elif quantization_bits == 8:\n",
    "                bnb_config.load_in_8bit = True\n",
    "            return bnb_config\n",
    "        return None\n",
    " \n",
    "\n",
    "    def _initialize_model_tokenizer(self, model_id: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "        \"\"\"\n",
    "        Initializes the model and tokenizer with the given model ID.\n",
    "        \"\"\"\n",
    "        model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_config.max_seq_len = self.model_max_length\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            quantization_config=self.bnb_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map='auto',\n",
    "        )\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, padding_side=\"left\", truncation_side=\"left\",\n",
    "            model_max_length=self.model_max_length\n",
    "        )\n",
    "        # Most LLMs don't have a pad token by default\n",
    "        tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "\n",
    "    def _define_stopping_criteria(self) -> StoppingCriteriaList:\n",
    "        \"\"\"\n",
    "        Defines stopping criteria for text generation based on the provided stop_list.\n",
    "        \"\"\"\n",
    "        stop_token_ids = [self.tokenizer(x)['input_ids'] for x in self.stop_list]\n",
    "        stop_token_ids = [torch.LongTensor(x).to(self.device) for x in stop_token_ids]\n",
    "\n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "                for stop_ids in stop_token_ids:\n",
    "                    if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        return StoppingCriteriaList([StopOnTokens()])\n",
    "    \n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 15) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt for generation.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: The generated text responses.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self.model_max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.1,\n",
    "            stopping_criteria=self.stopping_criteria,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "        llm_id, device, quantization_bits=4, \n",
    "        model_max_length=max_input_length\n",
    "    )\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(corpus_path)\n",
    "search_results = read_pickle(context_retrieval_dir)\n",
    "print(f\"Loaded {len(corpus)} records and {len(search_results)} search results for Top-{num_docs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted from chemdataextractor.text.normalize\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tools for normalizing text.\n",
    "https://github.com/mcs07/ChemDataExtractor\n",
    ":copyright: Copyright 2016 by Matt Swain.\n",
    ":license: MIT\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "'Software'), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
    "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "#: Control characters.\n",
    "CONTROLS = {\n",
    "    '\\u0001', '\\u0002', '\\u0003', '\\u0004', '\\u0005', '\\u0006', '\\u0007', '\\u0008', '\\u000e', '\\u000f', '\\u0011',\n",
    "    '\\u0012', '\\u0013', '\\u0014', '\\u0015', '\\u0016', '\\u0017', '\\u0018', '\\u0019', '\\u001a', '\\u001b',\n",
    "}\n",
    "# There are further control characters, but they are instead replaced with a space by unicode normalization\n",
    "# '\\u0009', '\\u000a', '\\u000b', '\\u000c', '\\u000d', '\\u001c',  '\\u001d', '\\u001e', '\\u001f'\n",
    "\n",
    "\n",
    "#: Hyphen and dash characters.\n",
    "HYPHENS = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '‐',  # \\u2010 Hyphen\n",
    "    '‑',  # \\u2011 Non-breaking hyphen\n",
    "    '⁃',  # \\u2043 Hyphen bullet\n",
    "    '‒',  # \\u2012 figure dash\n",
    "    '–',  # \\u2013 en dash\n",
    "    '—',  # \\u2014 em dash\n",
    "    '―',  # \\u2015 horizontal bar\n",
    "}\n",
    "\n",
    "#: Minus characters.\n",
    "MINUSES = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '−',  # \\u2212 Minus\n",
    "    '－',  # \\uff0d Full-width Hyphen-minus\n",
    "    '⁻',  # \\u207b Superscript minus\n",
    "}\n",
    "\n",
    "#: Plus characters.\n",
    "PLUSES = {\n",
    "    '+',  # \\u002b Plus\n",
    "    '＋',  # \\uff0b Full-width Plus\n",
    "    '⁺',  # \\u207a Superscript plus\n",
    "}\n",
    "\n",
    "#: Slash characters.\n",
    "SLASHES = {\n",
    "    '/',  # \\u002f Solidus\n",
    "    '⁄',  # \\u2044 Fraction slash\n",
    "    '∕',  # \\u2215 Division slash\n",
    "}\n",
    "\n",
    "#: Tilde characters.\n",
    "TILDES = {\n",
    "    '~',  # \\u007e Tilde\n",
    "    '˜',  # \\u02dc Small tilde\n",
    "    '⁓',  # \\u2053 Swung dash\n",
    "    '∼',  # \\u223c Tilde operator #in mbert vocab\n",
    "    '∽',  # \\u223d Reversed tilde\n",
    "    '∿',  # \\u223f Sine wave\n",
    "    '〜',  # \\u301c Wave dash #in mbert vocab\n",
    "    '～',  # \\uff5e Full-width tilde #in mbert vocab\n",
    "}\n",
    "\n",
    "#: Apostrophe characters.\n",
    "APOSTROPHES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '’',  # \\u2019\n",
    "    '՚',  # \\u055a\n",
    "    'Ꞌ',  # \\ua78b\n",
    "    'ꞌ',  # \\ua78c\n",
    "    '＇',  # \\uff07\n",
    "}\n",
    "\n",
    "#: Single quote characters.\n",
    "SINGLE_QUOTES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '‘',  # \\u2018\n",
    "    '’',  # \\u2019\n",
    "    '‚',  # \\u201a\n",
    "    '‛',  # \\u201b\n",
    "\n",
    "}\n",
    "\n",
    "#: Double quote characters.\n",
    "DOUBLE_QUOTES = {\n",
    "    '\"',  # \\u0022\n",
    "    '“',  # \\u201c\n",
    "    '”',  # \\u201d\n",
    "    '„',  # \\u201e\n",
    "    '‟',  # \\u201f\n",
    "}\n",
    "\n",
    "#: Accent characters.\n",
    "ACCENTS = {\n",
    "    '`',  # \\u0060\n",
    "    '´',  # \\u00b4\n",
    "}\n",
    "\n",
    "#: Prime characters.\n",
    "PRIMES = {\n",
    "    '′',  # \\u2032\n",
    "    '″',  # \\u2033\n",
    "    '‴',  # \\u2034\n",
    "    '‵',  # \\u2035\n",
    "    '‶',  # \\u2036\n",
    "    '‷',  # \\u2037\n",
    "    '⁗',  # \\u2057\n",
    "}\n",
    "\n",
    "#: Quote characters, including apostrophes, single quotes, double quotes, accents and primes.\n",
    "QUOTES = APOSTROPHES | SINGLE_QUOTES | DOUBLE_QUOTES | ACCENTS | PRIMES\n",
    "\n",
    "def normalize(text):\n",
    "    for control in CONTROLS:\n",
    "        text = text.replace(control, '')\n",
    "    text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n",
    "\n",
    "    for hyphen in HYPHENS | MINUSES:\n",
    "        text = text.replace(hyphen, '-')\n",
    "    text = text.replace('\\u00ad', '')\n",
    "\n",
    "    for double_quote in DOUBLE_QUOTES:\n",
    "        text = text.replace(double_quote, '\"')  # \\u0022\n",
    "    for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n",
    "        text = text.replace(single_quote, \"'\")  # \\u0027\n",
    "    text = text.replace('′', \"'\")     # \\u2032 prime\n",
    "    text = text.replace('‵', \"'\")     # \\u2035 reversed prime\n",
    "    text = text.replace('″', \"''\")    # \\u2033 double prime\n",
    "    text = text.replace('‶', \"''\")    # \\u2036 reversed double prime\n",
    "    text = text.replace('‴', \"'''\")   # \\u2034 triple prime\n",
    "    text = text.replace('‷', \"'''\")   # \\u2037 reversed triple prime\n",
    "    text = text.replace('⁗', \"''''\")  # \\u2057 quadruple prime\n",
    "\n",
    "    text = text.replace('…', '...').replace(' . . . ', ' ... ')  # \\u2026\n",
    "\n",
    "    for slash in SLASHES:\n",
    "        text = text.replace(slash, '/')\n",
    "\n",
    "    for tilde in TILDES:\n",
    "       text = text.replace(tilde, '~')\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_document(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SHA-256 hash for a given text.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        corpus: List[Dict],  \n",
    "        tokenizer: AutoTokenizer,\n",
    "        search_results: List[Tuple[List[int], List[float]]],\n",
    "        with_oracle: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.search_results = search_results\n",
    "        self.data_path = queries_path\n",
    "        self.max_tokenized_length = max_input_length - 2\n",
    "        self.do_normalize_query = normalize_queries\n",
    "        self.num_documents_in_context = num_docs\n",
    "        self.with_oracle = with_oracle\n",
    "\n",
    "        self._validate_initialization_parameters()\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "    def _validate_initialization_parameters(self):\n",
    "        if self.num_documents_in_context <= 0:\n",
    "            raise ValueError(\"num_documents_in_context must be positive.\")\n",
    "        \n",
    "        if self.max_tokenized_length <= 0:\n",
    "            raise ValueError(\"max_tokenized_length must be positive.\")\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        try:\n",
    "            with open(self.data_path, \"r\") as reader:\n",
    "                data = json.load(reader)\n",
    "            self.process_file_data(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {self.data_path}: {e}\")\n",
    "\n",
    "\n",
    "    def process_file_data(self, data: List[Dict]):  \n",
    "        \"\"\"\n",
    "        Processes each example in the dataset to prepare prompts for the LLM.\n",
    "\n",
    "        This involves assembling document contexts, normalizing text as needed,\n",
    "        and checking against the maximum token length to ensure compatibility with the LLM's input specifications.\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): The dataset, where each entry contains information about an example,\n",
    "            including the example's ID, the gold document index, answers, and the query.\n",
    "        \"\"\"\n",
    "        self.ids = []\n",
    "        self.queries = []\n",
    "        self.prompts = []\n",
    "        self.excluded_samples_ids = []\n",
    "        self.preprocessed_data = []\n",
    "        self.prompt_tokens_lengths = []\n",
    "\n",
    "        for i, sample in enumerate(data):\n",
    "            id = sample['_id']\n",
    "            query = sample['question']\n",
    "\n",
    "            formatted_documents, document_indices = [], []\n",
    "\n",
    "            if self.with_oracle:\n",
    "                oracle_docs = [{'title': title, 'text': ' '.join(text)} for title, text in sample['context']]\n",
    "                formatted_documents, document_indices = self._format_documents(oracle_docs, limit_context=False)\n",
    "            else:\n",
    "                # Retrieve the top-k documents for the query\n",
    "                formatted_documents, document_indices = self.prepare_documents_for_prompt(i)\n",
    "\n",
    "            # Normalize the query & build the prompt\n",
    "            documents_str = '\\n'.join(formatted_documents)\n",
    "            if self.do_normalize_query:\n",
    "                query = normalize(query)\n",
    "            prompt = self.build_qa_prompt(query, documents_str)\n",
    "\n",
    "            # Check if the prompt exceeds 'max_tokenized_length'\n",
    "            tokens = self.tokenizer.tokenize(prompt)\n",
    "            tokens_len = len(tokens)\n",
    "            if tokens_len >= self.max_tokenized_length:\n",
    "                self.excluded_samples_ids.append((i, id))\n",
    "                print(\"Skipping example {} due to prompt length.\".format((i, id)))\n",
    "                continue\n",
    "\n",
    "            if len(formatted_documents) < self.num_documents_in_context:\n",
    "                print(f\"Warning: Not enough documents for example {i}.\")\n",
    "\n",
    "            # If the prompt is valid, add it to the dataset\n",
    "            self.preprocessed_data.append((formatted_documents, list(document_indices)))\n",
    "            self.ids.append(id)\n",
    "            self.queries.append(query)\n",
    "            self.prompts.append(prompt)\n",
    "            self.prompt_tokens_lengths.append(tokens_len)\n",
    "\n",
    "\n",
    "    def prepare_documents_for_prompt(self, i: int) -> Tuple[List[str], List[int]]:\n",
    "        indices = self._get_indices(i) # Get the indices of the top-k documents\n",
    "        return self._get_documents_from_indices(indices)\n",
    "\n",
    "\n",
    "    def _get_indices(self, idx: int) -> List[int]:\n",
    "        indices, _ = self.search_results[idx]\n",
    "        return indices\n",
    "    \n",
    "\n",
    "    def _get_documents_from_indices(self, indices: List[int]) -> Tuple[List[str], List[int]]:\n",
    "        documents_info = [self.corpus[i] for i in map(int, indices)]\n",
    "        return self._format_documents(documents_info)\n",
    "    \n",
    "    def _format_documents(self, documents_info: List[Dict], limit_context: bool = True, with_corpus_id: bool = True) -> Tuple[List[str], List[int]]:\n",
    "        seen_hashes = set()\n",
    "        # List to store the indices of documents actually added\n",
    "        document_indices = []  \n",
    "        formatted_documents = []\n",
    "        for doc_info in documents_info:\n",
    "            if limit_context and len(formatted_documents) == self.num_documents_in_context:\n",
    "                break\n",
    "            \n",
    "            doc_idx = doc_info['full_corpus_idx'] if 'full_corpus_idx' in doc_info else -1\n",
    "            title = doc_info['title']\n",
    "            text = doc_info['text']\n",
    "\n",
    "            doc_hash = hash_document(text)\n",
    "            # Skip the document if it is a duplicate\n",
    "            if doc_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(doc_hash)\n",
    "            \n",
    "            doc_str = f\"Document [{doc_idx}](Title: {title}) {text}\"\n",
    "            formatted_documents.append(doc_str)\n",
    "            document_indices.append(doc_idx)\n",
    "\n",
    "        return formatted_documents, document_indices\n",
    "\n",
    "\n",
    "    def build_qa_prompt(self, query: str, documents_str: str) -> str:\n",
    "        task_instruction = \"You are given a question and you MUST respond by EXTRACTING the answer (max 5 tokens) from one of the provided documents. If none of the documents contain the answer, respond with NO-RES.\"\n",
    "        prompt = f\"\"\"{task_instruction}\\nDocuments:\\n{documents_str}\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "\n",
    "        # Custom prompt format for mpt models\n",
    "        if 'mpt' in self.tokenizer.name_or_path:\n",
    "            INSTRUCTION_KEY = \"### Instruction:\"\n",
    "            RESPONSE_KEY = \"### Response:\"\n",
    "            INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\"\"\".format(\n",
    "                intro=INTRO_BLURB,\n",
    "                instruction_key=INSTRUCTION_KEY,\n",
    "                instruction=\"{instruction}\",\n",
    "                response_key=RESPONSE_KEY,\n",
    "            )\n",
    "            prompt = PROMPT_FOR_GENERATION_FORMAT.format(\n",
    "                instruction=prompt[:-8]\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        _, document_indices = self.preprocessed_data[idx]\n",
    "\n",
    "        return {\n",
    "            \"id\": self.ids[idx],\n",
    "            \"query\": self.queries[idx],\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"document_indices\": document_indices,\n",
    "            \"prompt_tokens_len\": self.prompt_tokens_lengths[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset_and_loader(\n",
    "    corpus: List[Dict], \n",
    "    search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    with_oracle=False\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, \n",
    "        tokenizer=tokenizer,\n",
    "        search_results=search_results,\n",
    "        with_oracle=with_oracle\n",
    "    )\n",
    "    \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a question and you MUST respond by EXTRACTING the answer (max 5 tokens) from one of the provided documents. If none of the documents contain the answer, respond with NO-RES.\n",
      "Documents:\n",
      "Document [427816](Title: Is It Easy to Be Young?) Vai viegli būt jaunam? (Is It Easy to Be Young?) is a Soviet-era Latvian documentary film directed by Juris Podnieks. It was filmed in 1986 with dialog in both Latvian and Russian, and is considered to be among the most controversial movies of its era. It was one of the five winners of the 1987 International Documentary Association awards. The movie speaks about young people who perished as a result of growing up in Soviet society—their conflicts with parents and society, the patronizing attitudes of their teachers and the authorities, the fear that there is no meaning to their lives. Among the young people portrayed are high-schoolers looking for their place in life, a young mother worried about the future of her daughter after the Chernobyl catastrophe, a young man follower of the Hare Krishna movement (an 'unusual' religion that was discouraged even more than 'usual' ones by the Soviet government), as well young adults returning from compulsory military service in the Soviet–Afghan War and having become ones of 'the lost generation'. The film's opening scene documents a concert by the banned Latvian rock band, Pērkons. The movie had a major impact in the Soviet Union. It was seen by at least 28 million people during its first year. In all, 85 countries bought the rights to show the movie. For a Latvian film, this was an incredible number. In 1986 the film received Latvian Film Prize for the best documentary. Its international debut was at the 1987 Kraków Film Festival, where it received the FIPRESCI Award. It was also screened out of competition at the 1987 Cannes Film Festival. The film was re-released in 2007 by Jura Podnieka Studija; the new edition includes oral history interviews with Podnieks's colleagues. After Latvia regained independence, Antra Cilinska filmed two sequels (\"Is It Easy to Be?\" in 1998, and \"Is it Easy?\" in 2010), featuring interviews with people filmed by Podnieks.\n",
      "Document [442561](Title: Michał Waszyński) Michał Waszyński( 29 September 1904 – 20 February 1965) was first a film director in Poland, then in Italy, and later( as Michael Waszynski) a producer of the major American films, mainly in Spain. Known for his elegance and impeccable manners, he was, by the people who knew him, known as\" the prince\". Waszyński was born as Mosze Waks into a Polish Jewish family as Michał Waks in 1904 in Kowel, a small town in Volhynia( now in Ukraine), which at the time was part of Imperial Russia. As Germany occupied this part of Europe during World War I, he moved first to Warsaw and later to Berlin. As a young man he worked as an assistant director of the legendary German director F.W. Murnau. Upon his return to Poland he changed his name to Michał Waszyński and converted to Catholicism. In the 1930s Waszyński became the most prolific film director in Poland, directing 37 of the 147 films made in Poland in that decade, nearly one out of four. Along with the popular films in Polish directed to a wide local audience, he directed an important film in Yiddish\" The Dybbuk\", today a monument of the rich cultural life of East European Jewry before the Holocaust. At the beginning of World War II Waszyński escaped from Warsaw, which was being bombed by German planes, to Białystok. That city was taken in mid-September 1939 by the Germans, but within weeks, as a result of the Molotov- Ribbentrop pact, the city was given to the Soviet Union and occupied by its army. Waszyński began a new career as a theater director, first in Białystok and later in Moscow. In the summer of 1941, after Germany invaded the Soviet Union, Waszyński joined the newly formed Polish Army of general Władysław Anders( loyal to the Polish government in Exile in London) and subsequently was relocated to Persia( Iran), and later as a soldier of the 2nd Corps of the Polish Army to Egypt and Italy. As a member of the army film unit, he filmed the Battle of Monte Cassino, where the Polish Army suffered great losses, but helped to win the day. After World War II, he stayed in Italy, where he directed a Polish- language feature film about the Battle of Monte Cassino, and then three Italian films. Later in his career, Waszyński worked as a producer for the major American studios in Italy and( primarily) Spain, credited as\" Michael Waszynski\". His credits include\" The Quiet American\"( 1958)( associate producer),\" El Cid\"( 1961), and\" The Fall of the Roman Empire( film)\"( 1964)( executive producer and associate producer). He died of a heart attack on 20 February 1965 in Madrid, and was buried in a ceremonious Catholic funeral in Rome.\n",
      "Document [240277](Title: Michał Waszyński) Michał Waszyński (29 September 1904 – 20 February 1965) was first a film director in Poland, then in Italy, and later (as Michael Waszynski) a producer of the major American films, mainly in Spain. Known for his elegance and impeccable manners, he was, by the people who knew him, known as \"the prince\". Waszyński was born as Mosze Waks into a Polish Jewish family as Michał Waks in 1904 in Kowel, a small town in Volhynia (now in Ukraine), which at the time was part of Imperial Russia. As Germany occupied this part of Europe during World War I, he moved first to Warsaw and later to Berlin. As a young man he worked as an assistant director of the legendary German director F.W. Murnau. Upon his return to Poland he changed his name to Michał Waszyński and converted to Catholicism. In the 1930s Waszyński became the most prolific film director in Poland, directing 37 of the 147 films made in Poland in that decade, nearly one out of four. Along with the popular films in Polish directed to a wide local audience, he directed an important film in Yiddish \"The Dybbuk\", today a monument of the rich cultural life of East European Jewry before the Holocaust. At the beginning of World War II Waszyński escaped from Warsaw, which was being bombed by German planes, to Białystok. That city was taken in mid-September 1939 by the Germans, but within weeks, as a result of the Molotov-Ribbentrop pact, the city was given to the Soviet Union and occupied by its army. Waszyński began a new career as a theater director, first in Białystok and later in Moscow. In the summer of 1941, after Germany invaded the Soviet Union, Waszyński joined the newly formed Polish Army of general Władysław Anders (loyal to the Polish government in Exile in London) and subsequently was relocated to Persia (Iran), and later as a soldier of the 2nd Corps of the Polish Army to Egypt and Italy. As a member of the army film unit, he filmed the Battle of Monte Cassino, where the Polish Army suffered great losses, but helped to win the day. After World War II, he stayed in Italy, where he directed a Polish-language feature film about the Battle of Monte Cassino, and then three Italian films. Later in his career, Waszyński worked as a producer for the major American studios in Italy and (primarily) Spain, credited as \"Michael Waszynski\". His credits include \"The Quiet American\" (1958) (associate producer), \"El Cid\" (1961), and \"The Fall of the Roman Empire (film)\" (1964) (executive producer and associate producer). He died of a heart attack on 20 February 1965 in Madrid, and was buried in a ceremonious Catholic funeral in Rome.\n",
      "Document [235561](Title: Ashes and Diamonds (film)) Ashes and Diamonds( Polish: Popiół i diament) is a 1958 Polish drama film directed by Andrzej Wajda, based on the 1948 novel by Polish writer Jerzy Andrzejewski. Starring Zbigniew Cybulski and Ewa Krzyżewska, it completed Wajda's war films trilogy, following\" A Generation\"( 1954) and\" Kanal\"( 1956). The action of\" Ashes and Diamonds\" takes place in 1945, shortly after World War II. The main protagonist of the film, former Home Army soldier Maciek Chełmicki, is acting in the anti-Communist underground. Maciek receives an order to kill Szczuka, the local secretary of the Polish Workers' Party. Over time, Chełmicki increasingly doubts if his task is worth doing. \" Ashes and Diamonds\", although based on the novel which directly supported the postwar Communist system in Poland, was subtly modified in comparison with the source material. Wajda sympathized with the soldiers of the Polish independence underground; thus, he devoted most of the attention to Chełmicki. During the three- month development of\" Ashes and Diamonds\", the director made drastic changes to the baseline scenario, thanks to his assistant director Janusz Morgenstern, as well as Cybulski, who played the leading role. The film received permission from the authorities to be distributed only through Andrzejewski's intercession. The film did not receive permission to be screened at the main competition at the Cannes Film Festival. However,\" Ashes and Diamonds\" appeared at the Venice Film Festival, where it won the FIPRESCI award. At first,\" Ashes and Diamonds\" met positive critical reception, both in Poland and worldwide. However, after the Revolutions of 1989, it was criticized for falsifying the collective memory of Polish partisans. Nevertheless, the film maintained the reputation of one of the most famous Polish motion pictures in history.\n",
      "Document [248433](Title: Antoni Bohdziewicz) Antoni Bohdziewicz (September 11, 1906 – October 20, 1970) was a Polish screenplay writer and director, best known for his 1956 adaptation of \"Zemsta\" by Aleksander Fredro. Bohdziewicz was born in the city of Vilna (modern Vilnius), then part of the Russian Empire. In 1928, he graduated from the Technical Faculty of the Warsaw University of Technology and was simultaneously studying at the Faculty of Humanities of the Stefan Batory University. In 1928, he became a speaker at the newly established branch of the Polish Radio in his native city. In 1931 however he obtained a state scholarship and left for France. In Paris he joined the prestigious \"Ecole Technique de Photographie et de Cinématographie\", where he also made his first documentaries. In 1935, he returned to Poland and worked as a journalist and cameraman for the state-owned Polska Agencja Telegraficzna Film Chronicle (PAT), the most popular newsreel in Poland. He also worked as a journalist and columnist for the \"Pion\" weekly. In late 1930s he made numerous documentaries for the PAT agency, as well as for the SAF film studio. In 1939, he began working on his first feature film \"Zazdrość i medycyna\", based on a novel by Michał Choromański. However, the shooting was interrupted by the outbreak of the Invasion of Poland (1939). During World War II he was an active member of the Home Army and collaborated with the Bureau of Information and Propaganda as the head of the photo and film department. In 1943, he also started a \"Tres\" photographic studio in Warsaw, which became a clandestine outpost of the Home Army. During the Warsaw Uprising he became the head of the group of cameramen to prepare daily newsreels and was one of the people to prepare \"Warszawa walczy\", a documentary filmed and shown entirely in besieged Warsaw. After the war he continued his career in the same role and became one of the first members of the Polish Film Chronicle (PKF) company. Working in Kraków, already in March 1945 he started a Film Atelier for the Youth, the first film school to be opened in Poland after the end of the German occupation. In December of that year he converted his atelier into a regular study, which became a direct predecessor of the Kraków Film School. In 1948 he moved to Łódź, where he became the chairman of the Department of Direction of the National Film School. In that role he became a teacher of several generations of Polish film directors. He also remained an active director himself. His first film, \"2 *2=4\", was released already in 1945 and was among the first feature films to be shot in Poland after World War II. Between 1956 and 1962 Bohdziewicz served as an artistic director of the \"Droga Film Team\" and then the TOR Film Studio (1968–1970). Simultaneously he was also a teacher at the Brussels-based \"Institut National Supérieur des Arts du Spectacle\". He died October 20, 1970 in Warsaw.\n",
      "Document [454664](Title: Polish-Russian War (film)) Polish- Russian War( Wojna polsko- ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish- Russian War under the white- red flag by Dorota Masłowska.\n",
      "Document [507138](Title: Xawery Żuławski) Xawery Żuławski( born 22 December 1971 in Warsaw) is a Polish film director. In 1995 he graduated National Film School in Łódź. He is the son of actress Małgorzata Braunek and director Andrzej Żuławski. His second feature\" Wojna polsko- ruska\"( 2009), adapted from the controversial best- selling novel by Dorota Masłowska, won First Prize in the New Polish Films competition at the 9th Era New Horizons Film Festival in Wrocław. In 2013, he stated he intends to direct a Polish novel\" Zły\" by Leopold Tyrmand. Żuławski and his wife Maria Strzelecka had 2 children together: son Kaj Żuławski( born 2002) and daughter Jagna Żuławska( born 2009).\n",
      "Question: Who is the mother of the director of film Polish-Russian War (Film)?\n",
      "Answer:\n",
      "Initialized prompt dataset with 0 batches.\n"
     ]
    }
   ],
   "source": [
    "prompt_dataloader = initialize_dataset_and_loader(\n",
    "    corpus, search_results, tokenizer, True\n",
    ")\n",
    "print(f\"Initialized prompt dataset with {len(prompt_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info():\n",
    "    print(\"INFO:\")\n",
    "    print(f\"DATA: {queries_path}\")\n",
    "    print(f\"MODEL: {llm_id}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {num_docs}\")\n",
    "    print(f\"BATCH SIZE: {batch_size}\")\n",
    "    print(f\"SAVE EVERY: {save_every}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save(llm: LLM, prompt_dataloader: DataLoader):\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{llm_response_dir}/{llm_folder}/{num_docs}_doc\"\n",
    "    if not os.path.exists(saving_dir):\n",
    "        os.makedirs(saving_dir)\n",
    "\n",
    "    # MPT has a different answer string in the prompt\n",
    "    answer_string_in_prompt = \"### Response:\" if 'mpt' in llm_id else \"Answer:\"\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        with torch.no_grad():\n",
    "            generated_output = llm.generate(prompts, max_new_tokens=max_output_length)\n",
    "        \n",
    "        generated_answers = []\n",
    "        for output in generated_output:\n",
    "            start = output.find(answer_string_in_prompt) + len(answer_string_in_prompt)\n",
    "            response = output[start:].strip()\n",
    "            generated_answers.append(response)\n",
    "\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_docs}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        \n",
    "        del prompts, generated_output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "generate_and_save(llm, prompt_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"/kaggle/working/5_doc\", 'zip', \"/kaggle/working/Llama-2-7b-chat-hf/5_doc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
