{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use contriever to retrieve relevant documents per query; k = 1, 3, and 5\n",
    "  - Embed all documents in the corpus beforehand\n",
    "  - Embed the query during inference\n",
    "  - Retrieve the top k documents using cosine similarity\n",
    "2. Use the retrieved documents as context for the query (May need cleaning?)\n",
    "  - Cleaning: stop words, stemming, tokenization, etc. \n",
    "  - Concatenate the retrieved documents into a single string (are other methods better?)\n",
    "3. Provide the query and conext to a LLM to generate the answer (LLM from Huggingface)\n",
    "4. Compare the answer to the ground truth using exact match (or other metric(s))\n",
    "  - Exact match: answer == ground_truth\n",
    "  - Other metrics: ROUGE, BertScore, BLEU, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from transformers import PreTrainedModel, AutoConfig, AutoModel, AutoTokenizer\n",
    "from typing import List, Optional, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Turn off parallelism for tokenizers from Hugging Face\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "corpus_path = os.getenv(\"CORPUS_PATH\")\n",
    "encoder_id = os.getenv(\"ENCODER_ID\")\n",
    "max_length_encoder = int(os.getenv(\"MAX_LENGTH_ENCODER\"))\n",
    "normalize_embeddings = os.getenv(\"NORMALIZE_EMBEDDINGS\") == \"True\"\n",
    "lower_case = os.getenv(\"LOWER_CASE\") == \"True\"\n",
    "normalize_text = os.getenv(\"NORMALIZE_TEXT\") == \"True\"\n",
    "prefix_name = os.getenv(\"PREFIX_NAME\")\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "output_dir = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def read_json(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json(data, file_path: str):\n",
    "    with open(file_path, \"w\") as writer:\n",
    "        json.dump(data, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A wrapper class for encoding text using pre-trained transformer models with specified pooling strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: AutoConfig, pooling: str = \"average\"):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        if not hasattr(self.config, \"pooling\"):\n",
    "            self.config.pooling = pooling\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            config.name_or_path, config=self.config\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "    \n",
    "    def encode(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        normalize: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        model_output = self.forward(\n",
    "            input_ids, \n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "        )\n",
    "        last_hidden = model_output[\"last_hidden_state\"]\n",
    "        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.)\n",
    "\n",
    "        if self.config.pooling == \"average\":\n",
    "            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "        elif self.config.pooling == \"cls\":\n",
    "            emb = last_hidden[:, 0]\n",
    "\n",
    "        if normalize:\n",
    "            emb = F.normalize(emb, dim=-1)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \"\"\"\n",
    "    A class for retrieving document embeddings using a specified encoder, using a bi-encoder approach.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        query_encoder: Encoder,\n",
    "        doc_encoder: Optional[Encoder] = None,\n",
    "        max_length: int = 512,\n",
    "        add_special_tokens: bool = True,\n",
    "        norm_query_emb: bool = False,\n",
    "        norm_doc_emb: bool = False,\n",
    "        lower_case: bool = False,\n",
    "        do_normalize_text: bool = False,\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.query_encoder = query_encoder.to(device)\n",
    "        self.doc_encoder = self.query_encoder if doc_encoder is None else doc_encoder.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.norm_query_emb = norm_query_emb\n",
    "        self.norm_doc_emb = norm_doc_emb\n",
    "        self.lower_case = lower_case\n",
    "        self.do_normalize_text = do_normalize_text\n",
    "\n",
    "\n",
    "    def encode_queries(self, queries: List[str], batch_size: int) -> np.ndarray:\n",
    "        if self.do_normalize_text:\n",
    "            queries = [normalize_text.normalize(q) for q in queries]\n",
    "        if self.lower_case:\n",
    "            queries = [q.lower() for q in queries]\n",
    "\n",
    "        all_embeddings = []\n",
    "        nbatch = (len(queries) - 1) // batch_size + 1\n",
    "        with torch.no_grad():\n",
    "            for k in range(nbatch):\n",
    "                start_idx = k * batch_size\n",
    "                end_idx = min((k + 1) * batch_size, len(queries))\n",
    "\n",
    "                q_inputs = self.tokenizer(\n",
    "                    queries[start_idx:end_idx],\n",
    "                    max_length=self.max_length,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    add_special_tokens=self.add_special_tokens,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(self.device)\n",
    "\n",
    "                emb = self.query_encoder.encode(**q_inputs, normalize=self.norm_query_emb)\n",
    "                all_embeddings.append(emb.cpu())\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        return all_embeddings\n",
    "    \n",
    "\n",
    "    def encode_corpus(\n",
    "        self, \n",
    "        corpus_info: List[Dict[str, str]], \n",
    "        batch_size: int, \n",
    "        output_dir: str, \n",
    "        prefix_name: str,\n",
    "        save_every: int = 500\n",
    "    ) -> None:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        all_embeddings = []\n",
    "        num_steps = 0\n",
    "\n",
    "        nbatch = (len(corpus_info) - 1) // batch_size + 1\n",
    "        print(f\"Total number of batches: {nbatch}\")\n",
    "        with torch.no_grad():\n",
    "            for k in tqdm(range(nbatch)):\n",
    "                start_idx = k * batch_size\n",
    "                end_idx = min((k + 1) * batch_size, len(corpus_info))\n",
    "\n",
    "                corpus = [\n",
    "                    c[\"title\"] + \" \" + c[\"text\"] if len(c[\"title\"]) > 0 else c[\"text\"] \n",
    "                    for c in corpus_info[start_idx: end_idx]\n",
    "                ]\n",
    "                if self.do_normalize_text:\n",
    "                    corpus = [normalize_text.normalize(c) for c in corpus]\n",
    "                if self.lower_case:\n",
    "                    corpus = [c.lower() for c in corpus]\n",
    "\n",
    "                doc_inputs = self.tokenizer(\n",
    "                    corpus,\n",
    "                    max_length=self.max_length,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    add_special_tokens=self.add_special_tokens,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(self.device)\n",
    "\n",
    "                emb = self.doc_encoder.encode(**doc_inputs, normalize=self.norm_doc_emb)\n",
    "                all_embeddings.append(emb)\n",
    "\n",
    "                num_steps += 1\n",
    "\n",
    "                if num_steps == save_every or k == nbatch - 1:\n",
    "                    embeddings = torch.cat(all_embeddings, dim=0)\n",
    "                    file_index = end_idx - 1  # Index of the last passage embedded in the batch\n",
    "                    file_path = os.path.join(\n",
    "                        output_dir, f'{prefix_name}_{file_index}_embeddings.npy'\n",
    "                    )\n",
    "                    np.save(file_path, embeddings.cpu().numpy())\n",
    "                    print(f\"Saved embeddings for {file_index} passages.\")\n",
    "                    num_steps = 0\n",
    "                    all_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_retriever() -> Retriever:\n",
    "    config = AutoConfig.from_pretrained(encoder_id)\n",
    "    encoder = Encoder(config).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(encoder_id)\n",
    "    retriever = Retriever(\n",
    "        device=device, tokenizer=tokenizer, \n",
    "        query_encoder=encoder, \n",
    "        max_length=max_length_encoder,\n",
    "        norm_doc_emb=normalize_embeddings,\n",
    "        lower_case=lower_case,\n",
    "        do_normalize_text=normalize_text\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiaanbeekman/.pyenv/versions/3.12.1/envs/raspy_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "corpus = list(read_json(corpus_path).values())\n",
    "retriever = initialize_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.encode_corpus(\n",
    "    corpus, \n",
    "    batch_size=batch_size, \n",
    "    output_dir=output_dir,\n",
    "    prefix_name=prefix_name,\n",
    "    save_every=save_every\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
