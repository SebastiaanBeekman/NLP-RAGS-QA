{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries by LLM; Are not installed per default\n",
    "# ! pip install accelerate\n",
    "# ! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import regex\n",
    "import string\n",
    "import gc\n",
    "from numba import cuda\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, StoppingCriteriaList, StoppingCriteria,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Embeddings variables\n",
    "corpus_path = os.getenv(\"CORPUS_PATH\")\n",
    "\n",
    "# Retrieval variables\n",
    "queries_path = os.getenv(\"QUERIES_PATH\")\n",
    "\n",
    "## LLM variables\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "llm_id = os.getenv(\"LLM_ID\")\n",
    "num_docs = int(os.getenv(\"TOP_K\"))\n",
    "max_input_length = int(os.getenv(\"MAX_INPUT_LENGTH\"))\n",
    "max_output_length = int(os.getenv(\"MAX_OUTPUT_LENGTH\"))\n",
    "normalize_queries = os.getenv(\"NORMALIZE_QUERIES\") == \"True\"\n",
    "context_retrieval_dir = os.getenv(\"CONTEXT_RETRIEVAL_DIR\")\n",
    "llm_response_dir = os.getenv(\"LLM_RESPONSE_DIR\")\n",
    "\n",
    "# Make one true and the other one false\n",
    "USE_DOCUMENTS_FROM_CONTRIEVER = True\n",
    "USE_GOLD_DOCUMENTS_FROM_ORACLE = False\n",
    "\n",
    "# Add a mix of random documents or hard negatives to the retrieved document set\n",
    "NUMBER_OF_SAMPLED_RANDOM_DOCUMENTS = 0\n",
    "NUMBER_OF_HARD_NEGATIVES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = pickle.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_pickle(data, file_path: str):\n",
    "    with open(file_path, \"wb\") as writer:\n",
    "        pickle.dump(data, writer)\n",
    "\n",
    "\n",
    "def read_json(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data\n",
    "\n",
    "def read_corpus(corpus_path: str):\n",
    "    new_corpus = []\n",
    "    corpus = read_json(corpus_path).values()\n",
    "    for i, record in enumerate(corpus):\n",
    "        record[\"full_corpus_idx\"] = i\n",
    "        new_corpus.append(record)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run seeder before proceeding\n",
    "set_seeds(seed)\n",
    "print(f\"Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    \"\"\"\n",
    "    A class for loading and generating text using a Language Model (LM) with support for quantization\n",
    "    and custom stopping criteria.\n",
    "    \n",
    "    Attributes:\n",
    "        model_id (str): Identifier for the model to load.\n",
    "        device (str): Device to run the model on, e.g. 'cuda'.\n",
    "        quantization_bits (Optional[int]): Number of bits for quantization, supports 4 or 8 bits.\n",
    "        stop_list (Optional[List[str]]): List of tokens where generation should stop.\n",
    "        model_max_length (int): Maximum length of the model inputs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str, \n",
    "        device: str = 'cuda', \n",
    "        quantization_bits: Optional[int] = None, \n",
    "        stop_list: Optional[List[str]] = None, \n",
    "        model_max_length: int = 4096\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        self.stop_list = stop_list\n",
    "        if stop_list is None:\n",
    "            self.stop_list = ['\\nHuman:', '\\n```\\n', '\\nQuestion:', '<|endoftext|>', '\\n']\n",
    "        \n",
    "        self.bnb_config = self._set_quantization(quantization_bits)\n",
    "        self.model, self.tokenizer = self._initialize_model_tokenizer(model_id)\n",
    "        self.stopping_criteria = self._define_stopping_criteria()\n",
    "        \n",
    "\n",
    "    def _set_quantization(self, quantization_bits: Optional[int]) -> Optional[BitsAndBytesConfig]:\n",
    "        \"\"\"\n",
    "        Configure quantization settings based on the specified number of bits.\n",
    "        \"\"\"\n",
    "        if quantization_bits in [4, 8]:\n",
    "            bnb_config = BitsAndBytesConfig()\n",
    "            if quantization_bits == 4:\n",
    "                bnb_config.load_in_4bit = True\n",
    "                bnb_config.bnb_4bit_quant_type = 'nf4'\n",
    "                bnb_config.bnb_4bit_use_double_quant = True\n",
    "                bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            elif quantization_bits == 8:\n",
    "                bnb_config.load_in_8bit = True\n",
    "            return bnb_config\n",
    "        return None\n",
    " \n",
    "\n",
    "    def _initialize_model_tokenizer(self, model_id: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "        \"\"\"\n",
    "        Initializes the model and tokenizer with the given model ID.\n",
    "        \"\"\"\n",
    "        model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_config.max_seq_len = self.model_max_length\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            quantization_config=self.bnb_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map='auto',\n",
    "        )\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, padding_side=\"left\", truncation_side=\"left\",\n",
    "            model_max_length=self.model_max_length\n",
    "        )\n",
    "        # Most LLMs don't have a pad token by default\n",
    "        tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "\n",
    "    def _define_stopping_criteria(self) -> StoppingCriteriaList:\n",
    "        \"\"\"\n",
    "        Defines stopping criteria for text generation based on the provided stop_list.\n",
    "        \"\"\"\n",
    "        stop_token_ids = [self.tokenizer(x)['input_ids'] for x in self.stop_list]\n",
    "        stop_token_ids = [torch.LongTensor(x).to(self.device) for x in stop_token_ids]\n",
    "\n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "                for stop_ids in stop_token_ids:\n",
    "                    if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        return StoppingCriteriaList([StopOnTokens()])\n",
    "    \n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 15) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt for generation.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: The generated text responses.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self.model_max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.1,\n",
    "            stopping_criteria=self.stopping_criteria,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "        llm_id, device, quantization_bits=4, \n",
    "        model_max_length=max_input_length\n",
    "    )\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(corpus_path)\n",
    "search_results = read_pickle(context_retrieval_dir)\n",
    "print(f\"Loaded {len(corpus)} records and {len(search_results)} search results for Top-{num_docs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted from chemdataextractor.text.normalize\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tools for normalizing text.\n",
    "https://github.com/mcs07/ChemDataExtractor\n",
    ":copyright: Copyright 2016 by Matt Swain.\n",
    ":license: MIT\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "'Software'), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
    "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "#: Control characters.\n",
    "CONTROLS = {\n",
    "    '\\u0001', '\\u0002', '\\u0003', '\\u0004', '\\u0005', '\\u0006', '\\u0007', '\\u0008', '\\u000e', '\\u000f', '\\u0011',\n",
    "    '\\u0012', '\\u0013', '\\u0014', '\\u0015', '\\u0016', '\\u0017', '\\u0018', '\\u0019', '\\u001a', '\\u001b',\n",
    "}\n",
    "# There are further control characters, but they are instead replaced with a space by unicode normalization\n",
    "# '\\u0009', '\\u000a', '\\u000b', '\\u000c', '\\u000d', '\\u001c',  '\\u001d', '\\u001e', '\\u001f'\n",
    "\n",
    "\n",
    "#: Hyphen and dash characters.\n",
    "HYPHENS = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '‐',  # \\u2010 Hyphen\n",
    "    '‑',  # \\u2011 Non-breaking hyphen\n",
    "    '⁃',  # \\u2043 Hyphen bullet\n",
    "    '‒',  # \\u2012 figure dash\n",
    "    '–',  # \\u2013 en dash\n",
    "    '—',  # \\u2014 em dash\n",
    "    '―',  # \\u2015 horizontal bar\n",
    "}\n",
    "\n",
    "#: Minus characters.\n",
    "MINUSES = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '−',  # \\u2212 Minus\n",
    "    '－',  # \\uff0d Full-width Hyphen-minus\n",
    "    '⁻',  # \\u207b Superscript minus\n",
    "}\n",
    "\n",
    "#: Plus characters.\n",
    "PLUSES = {\n",
    "    '+',  # \\u002b Plus\n",
    "    '＋',  # \\uff0b Full-width Plus\n",
    "    '⁺',  # \\u207a Superscript plus\n",
    "}\n",
    "\n",
    "#: Slash characters.\n",
    "SLASHES = {\n",
    "    '/',  # \\u002f Solidus\n",
    "    '⁄',  # \\u2044 Fraction slash\n",
    "    '∕',  # \\u2215 Division slash\n",
    "}\n",
    "\n",
    "#: Tilde characters.\n",
    "TILDES = {\n",
    "    '~',  # \\u007e Tilde\n",
    "    '˜',  # \\u02dc Small tilde\n",
    "    '⁓',  # \\u2053 Swung dash\n",
    "    '∼',  # \\u223c Tilde operator #in mbert vocab\n",
    "    '∽',  # \\u223d Reversed tilde\n",
    "    '∿',  # \\u223f Sine wave\n",
    "    '〜',  # \\u301c Wave dash #in mbert vocab\n",
    "    '～',  # \\uff5e Full-width tilde #in mbert vocab\n",
    "}\n",
    "\n",
    "#: Apostrophe characters.\n",
    "APOSTROPHES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '’',  # \\u2019\n",
    "    '՚',  # \\u055a\n",
    "    'Ꞌ',  # \\ua78b\n",
    "    'ꞌ',  # \\ua78c\n",
    "    '＇',  # \\uff07\n",
    "}\n",
    "\n",
    "#: Single quote characters.\n",
    "SINGLE_QUOTES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '‘',  # \\u2018\n",
    "    '’',  # \\u2019\n",
    "    '‚',  # \\u201a\n",
    "    '‛',  # \\u201b\n",
    "\n",
    "}\n",
    "\n",
    "#: Double quote characters.\n",
    "DOUBLE_QUOTES = {\n",
    "    '\"',  # \\u0022\n",
    "    '“',  # \\u201c\n",
    "    '”',  # \\u201d\n",
    "    '„',  # \\u201e\n",
    "    '‟',  # \\u201f\n",
    "}\n",
    "\n",
    "#: Accent characters.\n",
    "ACCENTS = {\n",
    "    '`',  # \\u0060\n",
    "    '´',  # \\u00b4\n",
    "}\n",
    "\n",
    "#: Prime characters.\n",
    "PRIMES = {\n",
    "    '′',  # \\u2032\n",
    "    '″',  # \\u2033\n",
    "    '‴',  # \\u2034\n",
    "    '‵',  # \\u2035\n",
    "    '‶',  # \\u2036\n",
    "    '‷',  # \\u2037\n",
    "    '⁗',  # \\u2057\n",
    "}\n",
    "\n",
    "#: Quote characters, including apostrophes, single quotes, double quotes, accents and primes.\n",
    "QUOTES = APOSTROPHES | SINGLE_QUOTES | DOUBLE_QUOTES | ACCENTS | PRIMES\n",
    "\n",
    "def normalize(text):\n",
    "    for control in CONTROLS:\n",
    "        text = text.replace(control, '')\n",
    "    text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n",
    "\n",
    "    for hyphen in HYPHENS | MINUSES:\n",
    "        text = text.replace(hyphen, '-')\n",
    "    text = text.replace('\\u00ad', '')\n",
    "\n",
    "    for double_quote in DOUBLE_QUOTES:\n",
    "        text = text.replace(double_quote, '\"')  # \\u0022\n",
    "    for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n",
    "        text = text.replace(single_quote, \"'\")  # \\u0027\n",
    "    text = text.replace('′', \"'\")     # \\u2032 prime\n",
    "    text = text.replace('‵', \"'\")     # \\u2035 reversed prime\n",
    "    text = text.replace('″', \"''\")    # \\u2033 double prime\n",
    "    text = text.replace('‶', \"''\")    # \\u2036 reversed double prime\n",
    "    text = text.replace('‴', \"'''\")   # \\u2034 triple prime\n",
    "    text = text.replace('‷', \"'''\")   # \\u2037 reversed triple prime\n",
    "    text = text.replace('⁗', \"''''\")  # \\u2057 quadruple prime\n",
    "\n",
    "    text = text.replace('…', '...').replace(' . . . ', ' ... ')  # \\u2026\n",
    "\n",
    "    for slash in SLASHES:\n",
    "        text = text.replace(slash, '/')\n",
    "\n",
    "    for tilde in TILDES:\n",
    "       text = text.replace(tilde, '~')\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization adapted from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def remove_articles(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes articles ('a', 'an', 'the') from the text.\n",
    "    \"\"\"\n",
    "    return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "def white_space_fix(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes extra whitespace in the text by collapsing multiple spaces into one.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_punc(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the text and replaces it with a space.\n",
    "    \"\"\"\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' ')\n",
    "    return text\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts all characters in the text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_answer(s: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes answers by removing articles, punctuation, fixing whitespace, and optionally converting to lowercase.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        s = lower(s)\n",
    "    s = normalize(s)\n",
    "    return white_space_fix(remove_articles(remove_punc(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hard_negative_context(\n",
    "        context: List[List[Union[List[str],str]]],\n",
    "        evidence: List[List[str]],\n",
    "        question: str,\n",
    "        k = 2\n",
    "    ) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds the hard negative context from the context list that is not present in the evidence list.\n",
    "\n",
    "    The arguments for this function are such that it is possible to call this as follows when using the 2WikiMultiHopQA dataset:\n",
    "        retrieve_hard_negative_context(query['context'], query['evidences'], query['question'])\n",
    "\n",
    "    k is the number of hard negatives to be retrieved.\n",
    "    Hard negatives are retrieved from the oracle contexts.\n",
    "    \"\"\"\n",
    "    evidence_list = [[part1, part2] for [part1, _, part2] in evidence] # omit the relationship type\n",
    "    normalized_evidence_list_lower = [[normalize_answer(part1, lowercase=True),normalize_answer(part2, lowercase=True)] for [part1, part2] in evidence_list]\n",
    "    normalized_question = normalize_answer(question, lowercase=True)\n",
    "\n",
    "    hard_negative_context = []\n",
    "    cosine_similarity_scores = []\n",
    "\n",
    "    for [topic, contexts] in context:\n",
    "        normalized_context_lower = [normalize_answer(context, lowercase=True) for context in contexts]\n",
    "        normalized_context_lower.append(normalize_answer(topic, lowercase=True))\n",
    "        # concatenate the topic and context list to a single string\n",
    "        normalized_context_string = ' '.join(normalized_context_lower)\n",
    "        \n",
    "        context_relevant = False\n",
    "        for [part1, part2] in normalized_evidence_list_lower:\n",
    "            if part1 in normalized_context_string and part2 in normalized_context_string:\n",
    "                #print(\"relevant context found: \", part1, part2)\n",
    "                context_relevant = True\n",
    "                break\n",
    "            \n",
    "        if not context_relevant: \n",
    "            cosine_similarity_score = cosine_similarity(normalized_question, normalized_context_string)\n",
    "            cosine_similarity_scores.append(cosine_similarity_score)\n",
    "            hard_negative_context.append([topic, [contexts]])\n",
    "\n",
    "    # Now get the k hard negative contexts that have the highest dot product score with the question\n",
    "    # Sort the hard negative contexts by the cosine similarity score\n",
    "    hard_negative_context = [context for _, context in sorted(zip(cosine_similarity_scores, hard_negative_context), reverse=True)]\n",
    "    hard_negative_context = hard_negative_context[:k]\n",
    "\n",
    "    return hard_negative_context\n",
    "\n",
    "\n",
    "# implementation adapted from: https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/\n",
    "def cosine_similarity(X: str, Y: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two strings.\n",
    "    \"\"\"\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "    \n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "    \n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "    \n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "    \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return c + cosine\n",
    "\n",
    "def retrieve_golden_context(context, evidence: List[List[str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Finds the golden context from the context list that is not present in the evidence list.\n",
    "    \"\"\"\n",
    "    evidence_list = [[part1, part2] for [part1, _, part2] in evidence] # omit the relationship type\n",
    "    normalized_evidence_list_lower = [[normalize_answer(part1, lowercase=True),normalize_answer(part2, lowercase=True)] for [part1, part2] in evidence_list]\n",
    "\n",
    "    golden_context = []\n",
    "    for [topic, contexts] in context:\n",
    "        normalized_context_lower = [normalize_answer(context, lowercase=True) for context in contexts]\n",
    "        normalized_context_lower.append(normalize_answer(topic, lowercase=True))\n",
    "        # concatenate the topic and context list to a single string\n",
    "        context_string = ' '.join(normalized_context_lower)\n",
    "        \n",
    "        context_relevant = False\n",
    "        for [part1, part2] in normalized_evidence_list_lower:\n",
    "            if part1 in context_string and part2 in context_string:\n",
    "                #print(\"relevant context found: \", part1, part2)\n",
    "                golden_context.append([topic, [contexts]])\n",
    "                break\n",
    " \n",
    "    return golden_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_document(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SHA-256 hash for a given text.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        corpus: List[Dict],  \n",
    "        tokenizer: AutoTokenizer,\n",
    "        search_results: List[Tuple[List[int], List[float]]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.search_results = search_results\n",
    "        self.data_path = queries_path\n",
    "        self.max_tokenized_length = max_input_length - 2\n",
    "        self.do_normalize_query = normalize_queries\n",
    "        self.num_documents_in_context = num_docs\n",
    "    \n",
    "        \n",
    "        self._validate_initialization_parameters()\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "    def _validate_initialization_parameters(self):\n",
    "        if self.num_documents_in_context <= 0:\n",
    "            raise ValueError(\"num_documents_in_context must be positive.\")\n",
    "        \n",
    "        if self.max_tokenized_length <= 0:\n",
    "            raise ValueError(\"max_tokenized_length must be positive.\")\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        try:\n",
    "            with open(self.data_path, \"r\") as reader:\n",
    "                data = json.load(reader)\n",
    "            self.process_file_data(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {self.data_path}: {e}\")\n",
    "\n",
    "\n",
    "    def process_file_data(self, data: List[Dict]):  \n",
    "        \"\"\"\n",
    "        Processes each example in the dataset to prepare prompts for the LLM.\n",
    "\n",
    "        This involves assembling document contexts, normalizing text as needed,\n",
    "        and checking against the maximum token length to ensure compatibility with the LLM's input specifications.\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): The dataset, where each entry contains information about an example,\n",
    "            including the example's ID, the gold document index, answers, and the query.\n",
    "        \"\"\"\n",
    "        self.ids = []\n",
    "        self.queries = []\n",
    "        self.prompts = []\n",
    "        self.excluded_samples_ids = []\n",
    "        self.preprocessed_data = []\n",
    "        self.prompt_tokens_lengths = []\n",
    "\n",
    "        for i, sample in enumerate(data):\n",
    "            id = sample['_id']\n",
    "            query = sample['question']\n",
    "\n",
    "            all_formatted_documents: list[str] = []\n",
    "            expected_max_documents = 0\n",
    "\n",
    "            if USE_GOLD_DOCUMENTS_FROM_ORACLE & USE_GOLD_DOCUMENTS_FROM_ORACLE:\n",
    "                raise Exception(\"FATAL ERROR: using both contriever and gold documents is not allowed\")\n",
    "\n",
    "            if USE_DOCUMENTS_FROM_CONTRIEVER:\n",
    "                # Use contriever documents\n",
    "                formatted_relevant_documents, document_indices = self.prepare_relevant_documents_for_prompt(i)\n",
    "                all_formatted_documents.append(formatted_relevant_documents)\n",
    "                expected_max_documents += num_docs\n",
    "\n",
    "            if USE_GOLD_DOCUMENTS_FROM_ORACLE:\n",
    "                # Use gold documents\n",
    "                pass\n",
    "\n",
    "            # Retrieve the top-k documents for the query\n",
    "            if NUMBER_OF_SAMPLED_RANDOM_DOCUMENTS > 0:\n",
    "                formatted_random_documents, random_document_indices =  self.prepare_randomly_sampled_documents_for_prompt(NUMBER_OF_SAMPLED_RANDOM_DOCUMENTS)\n",
    "                all_formatted_documents.append(formatted_random_documents)\n",
    "                expected_max_documents += NUMBER_OF_SAMPLED_RANDOM_DOCUMENTS\n",
    "\n",
    "            if NUMBER_OF_HARD_NEGATIVES > 0:\n",
    "                # Add hard negatives\n",
    "                expected_max_documents += NUMBER_OF_HARD_NEGATIVES\n",
    "                pass\n",
    "\n",
    "            # Just add a random id when we don't have an ID\n",
    "\n",
    "            # Shuffle documents so that the random and relevant documents are not always in the same order\n",
    "            np.random.shuffle(all_formatted_documents)\n",
    "\n",
    "            # Normalize the query & build the prompt\n",
    "            documents_str = '\\n'.join(all_formatted_documents)\n",
    "            if self.do_normalize_query:\n",
    "                query = normalize(query)\n",
    "            prompt = self.build_qa_prompt(query, documents_str)\n",
    "\n",
    "            # Check if the prompt exceeds 'max_tokenized_length'\n",
    "            tokens = self.tokenizer.tokenize(prompt)\n",
    "            tokens_len = len(tokens)\n",
    "            if tokens_len >= self.max_tokenized_length:\n",
    "                self.excluded_samples_ids.append((i, id))\n",
    "                print(\"Skipping example {} due to prompt length.\".format((i, id)))\n",
    "                continue\n",
    "\n",
    "            if len(all_formatted_documents) < expected_max_documents:\n",
    "                print(f\"Warning: Not enough documents for example {i}.\")\n",
    "\n",
    "            # If the prompt is valid, add it to the dataset\n",
    "            self.preprocessed_data.append((all_formatted_documents, list(document_indices)))\n",
    "            self.ids.append(id)\n",
    "            self.queries.append(query)\n",
    "            self.prompts.append(prompt)\n",
    "            self.prompt_tokens_lengths.append(tokens_len)\n",
    "\n",
    "\n",
    "    def prepare_relevant_documents_for_prompt(self, i: int) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Returns the prepared documents for prompt with id i, based on the search results for that prompt id.\"\"\"\n",
    "        indices = self._get_indices(i) # Get the indices of the top-k documents\n",
    "        return self._get_documents_from_indices(map(int, indices))\n",
    "    \n",
    "    \n",
    "    def prepare_randomly_sampled_documents_for_prompt(self, n: int) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Randomly sample n documents from the corpus and format them appropriately for generation.\n",
    "\n",
    "        Returns the same format as prepare_relevant_documents_for_prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        random_indexes = np.random.choice(len(self.corpus), n)\n",
    "\n",
    "        return self._get_documents_from_indices(random_indexes)\n",
    "\n",
    "\n",
    "    def _get_indices(self, idx: int) -> List[int]:\n",
    "        \"\"\"Get the indices of the relevant documents for a given prompt as gathered by the retriever.\"\"\"\n",
    "        indices, _ = self.search_results[idx]\n",
    "        return indices\n",
    "    \n",
    "\n",
    "    def _format_document(self, index, title, text) -> str:\n",
    "        \"\"\"Format a document in the appropriate manner for prompt generation\"\"\"\n",
    "        return f\"Document [{index}](Title: {title}) {text}\"\n",
    "    \n",
    "\n",
    "    def _get_documents_from_indices(self, indices: List[int]) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Retrieve and format documents for prompt generation based on indices in the corpus.\"\"\"\n",
    "        documents_info = [self.corpus[i] for i in indices]\n",
    "        \n",
    "        seen_hashes = set()\n",
    "        # List to store the indices of documents actually added\n",
    "        document_indices = []  \n",
    "        formatted_documents = []\n",
    "        for doc_info in documents_info:\n",
    "            if len(formatted_documents) == self.num_documents_in_context:\n",
    "                break\n",
    "\n",
    "            doc_idx = doc_info['full_corpus_idx']\n",
    "            title = doc_info['title']\n",
    "            text = doc_info['text']\n",
    "\n",
    "            doc_hash = hash_document(text)\n",
    "            # Skip the document if it is a duplicate\n",
    "            if doc_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(doc_hash)\n",
    "\n",
    "            doc_str = self._format_document(doc_idx, title, text)\n",
    "            formatted_documents.append(doc_str)\n",
    "            document_indices.append(doc_idx)\n",
    "\n",
    "        return formatted_documents, document_indices\n",
    "\n",
    "\n",
    "    def build_qa_prompt(self, query: str, documents_str: str) -> str:\n",
    "        task_instruction = \"You are given a question and you MUST respond by EXTRACTING the answer (max 5 tokens) from one of the provided documents. If none of the documents contain the answer, respond with NO-RES.\"\n",
    "        prompt = f\"\"\"{task_instruction}\\nDocuments:\\n{documents_str}\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "\n",
    "        # Custom prompt format for mpt models\n",
    "        if 'mpt' in self.tokenizer.name_or_path:\n",
    "            INSTRUCTION_KEY = \"### Instruction:\"\n",
    "            RESPONSE_KEY = \"### Response:\"\n",
    "            INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\"\"\".format(\n",
    "                intro=INTRO_BLURB,\n",
    "                instruction_key=INSTRUCTION_KEY,\n",
    "                instruction=\"{instruction}\",\n",
    "                response_key=RESPONSE_KEY,\n",
    "            )\n",
    "            prompt = PROMPT_FOR_GENERATION_FORMAT.format(\n",
    "                instruction=prompt[:-8]\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        _, document_indices = self.preprocessed_data[idx]\n",
    "\n",
    "        return {\n",
    "            \"id\": self.ids[idx],\n",
    "            \"query\": self.queries[idx],\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"document_indices\": document_indices,\n",
    "            \"prompt_tokens_len\": self.prompt_tokens_lengths[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset_and_loader(\n",
    "    corpus: List[Dict], \n",
    "    search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, \n",
    "        tokenizer=tokenizer,\n",
    "        search_results=search_results\n",
    "    )\n",
    "    \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataloader = initialize_dataset_and_loader(\n",
    "    corpus, search_results, tokenizer\n",
    ")\n",
    "print(f\"Initialized prompt dataset with {len(prompt_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info():\n",
    "    print(\"INFO:\")\n",
    "    print(f\"DATA: {queries_path}\")\n",
    "    print(f\"MODEL: {llm_id}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {num_docs}\")\n",
    "    print(f\"BATCH SIZE: {batch_size}\")\n",
    "    print(f\"SAVE EVERY: {save_every}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save(llm: LLM, prompt_dataloader: DataLoader):\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{llm_response_dir}/{llm_folder}/{num_docs}_doc\"\n",
    "    if not os.path.exists(saving_dir):\n",
    "        os.makedirs(saving_dir)\n",
    "\n",
    "    # MPT has a different answer string in the prompt\n",
    "    answer_string_in_prompt = \"### Response:\" if 'mpt' in llm_id else \"Answer:\"\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        with torch.no_grad():\n",
    "            generated_output = llm.generate(prompts, max_new_tokens=max_output_length)\n",
    "        \n",
    "        generated_answers = []\n",
    "        for output in generated_output:\n",
    "            start = output.find(answer_string_in_prompt) + len(answer_string_in_prompt)\n",
    "            response = output[start:].strip()\n",
    "            generated_answers.append(response)\n",
    "\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_docs}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        \n",
    "        # Memory maintenance\n",
    "        del prompts, generated_output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "generate_and_save(llm, prompt_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"/kaggle/working/docs1_random_1\", 'zip', \"/kaggle/working/llm_responses/Llama-2-7b-chat-hf/3_doc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
