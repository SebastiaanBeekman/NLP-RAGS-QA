{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Embedding variables\n",
    "embeddings_dir = os.getenv(\"EMBEDDINGS_DIR\")\n",
    "\n",
    "# Indexing variables\n",
    "corpus_size = int(os.getenv(\"CORPUS_SIZE\"))\n",
    "vector_size = int(os.getenv(\"VECTOR_SIZE\"))\n",
    "faiss_dir = os.getenv(\"FAISS_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = pickle.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_pickle(data, file_path: str):\n",
    "    with open(file_path, \"wb\") as writer:\n",
    "        pickle.dump(data, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run seeder before proceeding\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings are stored in multiple files due to batch size, this function saves all embeddings into a single file\n",
    "def load_all_embeddings() -> np.array:\n",
    "    all_embeddings_path = f'{embeddings_dir}/all_embeddings.npy'\n",
    "\n",
    "    # Check if the file with all embeddings already exists and in case load it\n",
    "    if os.path.isfile(all_embeddings_path):\n",
    "        embeddings = np.load(all_embeddings_path, mmap_mode='c')\n",
    "        return embeddings\n",
    "\n",
    "    all_embeddings = []\n",
    "    num_embed = batch_size * save_every\n",
    "\n",
    "    for i in range(num_embed - 1, corpus_size, num_embed):\n",
    "        emb_path = f'{embeddings_dir}/{i}_embeddings.npy'\n",
    "        emb = np.load(emb_path, mmap_mode='c')\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    last_idx = corpus_size - 1\n",
    "    last_emb_path = f'{embeddings_dir}/{last_idx}_embeddings.npy'\n",
    "    last_emb = np.load(last_emb_path, mmap_mode='c')\n",
    "    all_embeddings.append(last_emb)\n",
    "\n",
    "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    np.save(all_embeddings_path, embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings for indexer\n",
    "embeddings = load_all_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(object):\n",
    "    def __init__(self, vector_size: int):     \n",
    "        self.index = faiss.IndexFlatIP(vector_size)\n",
    "        self.index_id_to_db_id = []\n",
    "\n",
    "\n",
    "    def index_data(self, ids: List[int], embeddings: np.array):\n",
    "        \"\"\"\n",
    "        Adds data to the index.\n",
    "\n",
    "        Args:\n",
    "            ids (List[int]): A list of database IDs corresponding to the embeddings.\n",
    "            embeddings (np.array): A numpy array of embeddings to be indexed.\n",
    "        \"\"\"\n",
    "        self._update_id_mapping(ids)\n",
    "        # embeddings = embeddings.astype('float32')\n",
    "        if not self.index.is_trained:\n",
    "            self.index.train(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        print(f'Total data indexed {len(self.index_id_to_db_id)}')\n",
    "\n",
    "    def serialize(\n",
    "        self, \n",
    "        dir_path: str, \n",
    "        index_file_name: Optional[str] = None, \n",
    "        meta_file_name: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Serializes the index and its metadata to disk.\n",
    "\n",
    "        Args:\n",
    "            dir_path (str): The directory path to save the serialized index and metadata.\n",
    "            index_file_name (Optional[str]): Optional custom name for the index file.\n",
    "            meta_file_name (Optional[str]): Optional custom name for the metadata file.\n",
    "        \"\"\"\n",
    "        if index_file_name is None:\n",
    "            index_file_name = 'index.faiss'\n",
    "        if meta_file_name is None:\n",
    "            meta_file_name = 'index_meta.faiss'\n",
    "\n",
    "        index_file = os.path.join(dir_path, index_file_name)\n",
    "        meta_file = os.path.join(dir_path, meta_file_name)\n",
    "        print(f'Serializing index to {index_file}, meta data to {meta_file}')\n",
    "\n",
    "        faiss.write_index(self.index, index_file)\n",
    "        write_pickle(self.index_id_to_db_id, meta_file)\n",
    "        \n",
    "\n",
    "    def _update_id_mapping(self, db_ids: List[int]):\n",
    "        self.index_id_to_db_id.extend(db_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_embeddings(embeddings: np.array) -> None:\n",
    "    os.makedirs(faiss_dir, exist_ok=True)\n",
    "\n",
    "    index = Indexer(vector_size)\n",
    "    index.index_data(list(range(corpus_size)), embeddings)\n",
    "\n",
    "    index.serialize(\n",
    "        dir_path=faiss_dir, \n",
    "        index_file_name=f'index.faiss', \n",
    "        meta_file_name=f'index_meta.faiss'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data indexed 563424\n",
      "Serializing index to ../data/embeddings/indexes/index.faiss, meta data to ../data/embeddings/indexes/index_meta.faiss\n"
     ]
    }
   ],
   "source": [
    "indexing_embeddings(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
