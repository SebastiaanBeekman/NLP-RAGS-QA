{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\minou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import regex\n",
    "import string\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Retrieval variables\n",
    "queries_path = os.getenv(\"QUERIES_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted from chemdataextractor.text.normalize\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tools for normalizing text.\n",
    "https://github.com/mcs07/ChemDataExtractor\n",
    ":copyright: Copyright 2016 by Matt Swain.\n",
    ":license: MIT\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "'Software'), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
    "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "#: Control characters.\n",
    "CONTROLS = {\n",
    "    '\\u0001', '\\u0002', '\\u0003', '\\u0004', '\\u0005', '\\u0006', '\\u0007', '\\u0008', '\\u000e', '\\u000f', '\\u0011',\n",
    "    '\\u0012', '\\u0013', '\\u0014', '\\u0015', '\\u0016', '\\u0017', '\\u0018', '\\u0019', '\\u001a', '\\u001b',\n",
    "}\n",
    "# There are further control characters, but they are instead replaced with a space by unicode normalization\n",
    "# '\\u0009', '\\u000a', '\\u000b', '\\u000c', '\\u000d', '\\u001c',  '\\u001d', '\\u001e', '\\u001f'\n",
    "\n",
    "\n",
    "#: Hyphen and dash characters.\n",
    "HYPHENS = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '‐',  # \\u2010 Hyphen\n",
    "    '‑',  # \\u2011 Non-breaking hyphen\n",
    "    '⁃',  # \\u2043 Hyphen bullet\n",
    "    '‒',  # \\u2012 figure dash\n",
    "    '–',  # \\u2013 en dash\n",
    "    '—',  # \\u2014 em dash\n",
    "    '―',  # \\u2015 horizontal bar\n",
    "}\n",
    "\n",
    "#: Minus characters.\n",
    "MINUSES = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '−',  # \\u2212 Minus\n",
    "    '－',  # \\uff0d Full-width Hyphen-minus\n",
    "    '⁻',  # \\u207b Superscript minus\n",
    "}\n",
    "\n",
    "#: Plus characters.\n",
    "PLUSES = {\n",
    "    '+',  # \\u002b Plus\n",
    "    '＋',  # \\uff0b Full-width Plus\n",
    "    '⁺',  # \\u207a Superscript plus\n",
    "}\n",
    "\n",
    "#: Slash characters.\n",
    "SLASHES = {\n",
    "    '/',  # \\u002f Solidus\n",
    "    '⁄',  # \\u2044 Fraction slash\n",
    "    '∕',  # \\u2215 Division slash\n",
    "}\n",
    "\n",
    "#: Tilde characters.\n",
    "TILDES = {\n",
    "    '~',  # \\u007e Tilde\n",
    "    '˜',  # \\u02dc Small tilde\n",
    "    '⁓',  # \\u2053 Swung dash\n",
    "    '∼',  # \\u223c Tilde operator #in mbert vocab\n",
    "    '∽',  # \\u223d Reversed tilde\n",
    "    '∿',  # \\u223f Sine wave\n",
    "    '〜',  # \\u301c Wave dash #in mbert vocab\n",
    "    '～',  # \\uff5e Full-width tilde #in mbert vocab\n",
    "}\n",
    "\n",
    "#: Apostrophe characters.\n",
    "APOSTROPHES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '’',  # \\u2019\n",
    "    '՚',  # \\u055a\n",
    "    'Ꞌ',  # \\ua78b\n",
    "    'ꞌ',  # \\ua78c\n",
    "    '＇',  # \\uff07\n",
    "}\n",
    "\n",
    "#: Single quote characters.\n",
    "SINGLE_QUOTES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '‘',  # \\u2018\n",
    "    '’',  # \\u2019\n",
    "    '‚',  # \\u201a\n",
    "    '‛',  # \\u201b\n",
    "\n",
    "}\n",
    "\n",
    "#: Double quote characters.\n",
    "DOUBLE_QUOTES = {\n",
    "    '\"',  # \\u0022\n",
    "    '“',  # \\u201c\n",
    "    '”',  # \\u201d\n",
    "    '„',  # \\u201e\n",
    "    '‟',  # \\u201f\n",
    "}\n",
    "\n",
    "#: Accent characters.\n",
    "ACCENTS = {\n",
    "    '`',  # \\u0060\n",
    "    '´',  # \\u00b4\n",
    "}\n",
    "\n",
    "#: Prime characters.\n",
    "PRIMES = {\n",
    "    '′',  # \\u2032\n",
    "    '″',  # \\u2033\n",
    "    '‴',  # \\u2034\n",
    "    '‵',  # \\u2035\n",
    "    '‶',  # \\u2036\n",
    "    '‷',  # \\u2037\n",
    "    '⁗',  # \\u2057\n",
    "}\n",
    "\n",
    "#: Quote characters, including apostrophes, single quotes, double quotes, accents and primes.\n",
    "QUOTES = APOSTROPHES | SINGLE_QUOTES | DOUBLE_QUOTES | ACCENTS | PRIMES\n",
    "\n",
    "def normalize(text):\n",
    "    for control in CONTROLS:\n",
    "        text = text.replace(control, '')\n",
    "    text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n",
    "\n",
    "    for hyphen in HYPHENS | MINUSES:\n",
    "        text = text.replace(hyphen, '-')\n",
    "    text = text.replace('\\u00ad', '')\n",
    "\n",
    "    for double_quote in DOUBLE_QUOTES:\n",
    "        text = text.replace(double_quote, '\"')  # \\u0022\n",
    "    for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n",
    "        text = text.replace(single_quote, \"'\")  # \\u0027\n",
    "    text = text.replace('′', \"'\")     # \\u2032 prime\n",
    "    text = text.replace('‵', \"'\")     # \\u2035 reversed prime\n",
    "    text = text.replace('″', \"''\")    # \\u2033 double prime\n",
    "    text = text.replace('‶', \"''\")    # \\u2036 reversed double prime\n",
    "    text = text.replace('‴', \"'''\")   # \\u2034 triple prime\n",
    "    text = text.replace('‷', \"'''\")   # \\u2037 reversed triple prime\n",
    "    text = text.replace('⁗', \"''''\")  # \\u2057 quadruple prime\n",
    "\n",
    "    text = text.replace('…', '...').replace(' . . . ', ' ... ')  # \\u2026\n",
    "\n",
    "    for slash in SLASHES:\n",
    "        text = text.replace(slash, '/')\n",
    "\n",
    "    for tilde in TILDES:\n",
    "       text = text.replace(tilde, '~')\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization adapted from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def remove_articles(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes articles ('a', 'an', 'the') from the text.\n",
    "    \"\"\"\n",
    "    return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "def white_space_fix(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes extra whitespace in the text by collapsing multiple spaces into one.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_punc(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the text and replaces it with a space.\n",
    "    \"\"\"\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' ')\n",
    "    return text\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts all characters in the text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_answer(s: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes answers by removing articles, punctuation, fixing whitespace, and optionally converting to lowercase.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        s = lower(s)\n",
    "    s = normalize(s)\n",
    "    return white_space_fix(remove_articles(remove_punc(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hard_negative_context(context: List[List[Union[List[str],str]]], evidence: List[List[str]], question: str, k = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Finds the hard negative context from the context list that is not present in the evidence list.\n",
    "    \"\"\"\n",
    "    evidence_list = [[part1, part2] for [part1, _, part2] in evidence] # omit the relationship type\n",
    "    normalized_evidence_list_lower = [[normalize_answer(part1, lowercase=True),normalize_answer(part2, lowercase=True)] for [part1, part2] in evidence_list]\n",
    "    normalized_question = normalize_answer(question, lowercase=True)\n",
    "    #print(evidence_list)\n",
    "    print(question)\n",
    "\n",
    "    hard_negative_context = []\n",
    "    cosine_similarity_scores = []\n",
    "\n",
    "    for [topic, contexts] in context:\n",
    "        normalized_context_lower = [normalize_answer(context, lowercase=True) for context in contexts]\n",
    "        normalized_context_lower.append(normalize_answer(topic, lowercase=True))\n",
    "        # concatenate the topic and context list to a single string\n",
    "        normalized_context_string = ' '.join(normalized_context_lower)\n",
    "        \n",
    "        context_relevant = False\n",
    "        for [part1, part2] in normalized_evidence_list_lower:\n",
    "            if part1 in normalized_context_string and part2 in normalized_context_string:\n",
    "                #print(\"relevant context found: \", part1, part2)\n",
    "                context_relevant = True\n",
    "                break\n",
    "            \n",
    "        if not context_relevant: \n",
    "            cosine_similarity_score = cosine_similarity(normalized_question, normalized_context_string)\n",
    "            cosine_similarity_scores.append(cosine_similarity_score)\n",
    "            hard_negative_context.append([topic, [contexts]])\n",
    "            print(len(normalized_context_string), cosine_similarity_score,normalized_context_string)\n",
    "            \n",
    "  \n",
    "    # Now get the k hard negative contexts that have the highest dot product score with the question\n",
    "    # Sort the hard negative contexts by the cosine similarity score\n",
    "    hard_negative_context = [context for _, context in sorted(zip(cosine_similarity_scores, hard_negative_context), reverse=True)]\n",
    "    hard_negative_context = hard_negative_context[:k]\n",
    "    \n",
    "    return hard_negative_context\n",
    "\n",
    "\n",
    "# implementation adapted from: https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/\n",
    "def cosine_similarity(X: str, Y: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two strings.\n",
    "    \"\"\"\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "    \n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "    \n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "    \n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "    \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return c + cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_golden_context(context, evidence: List[List[str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Finds the golden context from the context list that is not present in the evidence list.\n",
    "    \"\"\"\n",
    "    evidence_list = [[part1, part2] for [part1, _, part2] in evidence] # omit the relationship type\n",
    "    normalized_evidence_list_lower = [[normalize_answer(part1, lowercase=True),normalize_answer(part2, lowercase=True)] for [part1, part2] in evidence_list]\n",
    "    #print(evidence_list)\n",
    "\n",
    "    golden_context = []\n",
    "    for [topic, contexts] in context:\n",
    "        normalized_context_lower = [normalize_answer(context, lowercase=True) for context in contexts]\n",
    "        normalized_context_lower.append(normalize_answer(topic, lowercase=True))\n",
    "        # concatenate the topic and context list to a single string\n",
    "        context_string = ' '.join(normalized_context_lower)\n",
    "        \n",
    "        context_relevant = False\n",
    "        for [part1, part2] in normalized_evidence_list_lower:\n",
    "            if part1 in context_string and part2 in context_string:\n",
    "                #print(\"relevant context found: \", part1, part2)\n",
    "                golden_context.append([topic, [contexts]])\n",
    "                break\n",
    " \n",
    "    return golden_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the mother of the director of film Polish-Russian War (Film)?\n",
      "323 1.0758098043578903 maheen khan is pakistani fashion and costume designer also award winner fashion designer for fashion labels like embroidery housemaheen and gulabo she has done many national and international fashion events and shows she undertook embroidery for film snow white and huntsman and television series jewel in crown maheen khan\n",
      "200 2.174077655955698 viktor petrovich yeliseyev born june 9 1950 is russian general orchestra conductor and music teacher he is director of ministry of interior ensemble one of two russian red army choirs viktor yeliseyev\n",
      "224 1.087038827977849 alice washburn 1860 1929 was american stage and film actress she worked at edison vitagraph and kalem studios her final film snow white was her only known feature film she died of heart attack in november 1929 alice washburn\n",
      "53 1.1825741858350554 she was mother of prince morinaga minamoto no chikako\n",
      "609 1.0560772154092044 snow white christmas is christmas animated television special produced by filmation and telecast december 19 1980 on cbs it is sequel to fairy tale snow white unrelated to filmation s other sequel to snow white titled happily ever after 1990 film s plot revolves around return of wicked queen who is unexpectedly brought back to life during christmas and casts evil spell that freezes entire land only young snow white daughter of original snow white manages to escape and take refuge with seven giants with her dwarf friend it is now up to giants to defeat queen forever and save kingdom snow white christmas\n",
      "734 1.0506369683541834 snow white and three stooges is second feature film to star three stooges after their 1959 resurgence in popularity by this time trio consisted of moe howard larry fine and joe derita dubbed curly joe released by 20th century fox this was trio s take on classic fairy tale snow white and seven dwarfs film was retitled snow white and three clowns in great britain this was walter lang s final directing film before his retirement olympic gold medalist figure skater carol heiss starred as snow white who must flee her home after evil queen her evil stepmother wishes her to be dead seeking refuge in cottage of seven dwarfs she accidentally meets stooges who are house sitting for them while they are away snow white and three stooges\n",
      "176 1.1020620726159658 snow white and seven dwarfs usa snow white is 1955 german film directed by erich kobler based on story of schneewittchen by brothers grimm snow white and seven dwarfs 1955 film\n",
      "348 1.0690065559342354 liberty lettice lark ross born 23 september 1978 is english model and actress she has appeared in publications such as vogueharper s bazaari d and dazed confused she played role of queen eleanor in 2012 fantasy film snow white and huntsman directed by her then husband rupert sanders she is sister of composers atticus and leopold ross liberty ross\n",
      "hard_negative_context:  [['Viktor Yeliseyev', [['Viktor Petrovich Yeliseyev( born June 9, 1950) is a Russian general, orchestra conductor and music teacher.', 'He is the director of the Ministry of the Interior Ensemble, one of the two Russian Red Army Choirs.']]], ['Minamoto no Chikako', [['She was the mother of Prince Morinaga.']]]]\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?\n",
      "302 3.2314550249431377 waldemar i prince of anhalt zerbst died 7 january 1368 was german prince of house of ascania and ruler of principality of anhalt zerbst he was youngest son of albert i prince of anhalt zerbst by his second wife agnes daughter of conrad margrave of brandenburg stendal waldemar i prince of anhalt zerbst\n",
      "343 5.345032779671177 bernhard vii of anhalt zerbst 17 march 1540 1 march 1570 was german prince of house of ascania and ruler of principality of anhalt zerbst he was born and died in dessau and was third and youngest son of john v prince of anhalt zerbst by his wife margaret daughter of joachim i nestor elector of brandenburg bernhard vii prince of anhalt zerbst\n",
      "321 3.2142857142857144 albert iii prince of anhalt zerbst died ca 1 august 1359 was german prince of house of ascania and ruler of principality of anhalt zerbst he was eldest son of albert ii prince of anhalt zerbst by his second wife beatrix daughter of rudolf i elector of saxony and duke of saxe wittemberg albert iii prince of anhalt zerbst\n",
      "297 4.3023715784073815 john vi of anhalt zerbst zerbst 24 march 1621 zerbst 4 july 1667 was german prince of house of ascania and ruler of principality of anhalt zerbst he was only son of rudolph prince of anhalt zerbst by his second wife magdalene daughter of john vii count of oldenburg john vi prince of anhalt zerbst\n",
      "317 3.2182178902359926 albert ii prince of anhalt zerbst died 17 july 1362 was german prince of house of ascania and ruler of principality of anhalt zerbst he was third son of albert i prince of anhalt zerbst but eldest child born to his second wife agnes daughter of conrad margrave of brandenburg stendal albert ii prince of anhalt zerbst\n",
      "314 4.2909571869813234 john ii prince of anhalt zerbst died 11 april 1382 was german prince of house of ascania and ruler of principality of anhalt zerbst he was youngest son of albert ii prince of anhalt zerbst by his second wife beatrix daughter of rudolf i elector of saxony and duke of saxe wittemberg john ii prince of anhalt zerbst\n",
      "292 4.322329185610152 waldemar iii prince of anhalt zerbst died 1391 was german prince of house of ascania and ruler of principality of anhalt zerbst he was youngest son of john ii prince of anhalt zerbst by his wife elisabeth daughter of john i count of henneberg schleusingen waldemar iii prince of anhalt zerbst\n",
      "308 5.363696483726654 karl i of anhalt zerbst 17 november 1534 in dessau 4 may 1561 in zerbst was german prince of house of ascania and ruler of principality of anhalt zerbst he was eldest son of john v prince of anhalt zerbst by his wife margaret daughter of joachim i nestor elector of brandenburg karl i prince of anhalt zerbst\n",
      "hard_negative_context:  [['Karl I, Prince of Anhalt-Zerbst', [['Karl I of Anhalt- Zerbst( 17 November 1534 in Dessau – 4 May 1561 in Zerbst), was a German prince of the House of Ascania and ruler of the principality of Anhalt- Zerbst.', 'He was the eldest son of John V, Prince of Anhalt- Zerbst, by his wife Margaret, daughter of Joachim I Nestor, Elector of Brandenburg.']]], ['Bernhard VII, Prince of Anhalt-Zerbst', [['Bernhard VII of Anhalt- Zerbst( 17 March 1540 – 1 March 1570), was a German prince of the House of Ascania and ruler of the principality of Anhalt- Zerbst.', 'He was born and died in Dessau, and was the third and youngest son of John V, Prince of Anhalt- Zerbst by his wife Margaret, daughter of Joachim I Nestor, Elector of Brandenburg.']]]]\n",
      "What is the award that the director of film Wearing Velvet Slippers Under A Golden Umbrella won?\n",
      "174 1.0811107105653812 michael govan born 1963 is director of los angeles county museum of art since 2006 prior to this govan worked as director of dia art foundation in new york city michael govan\n",
      "75 2.2672612419124243 peter levin is american director of film television and theatre peter levin\n",
      "64 1.1443375672974065 john donatich is director of yale university press john donatich\n",
      "163 2.1889822365046134 dana blankstein cohen born march 3 1981 is director of israeli academy of film and television she is film director and israeli culture entrepreneur dana blankstein\n",
      "66 2.288675134594813 ian barry is australian director of film and tv ian barry director\n",
      "77 1.1336306209562121 john farrell is director of youtube in latin america john farrell businessman\n",
      "1469 0.0 etan boritzer born 1950 is american writer of children s literature who is best known for his book what is god first published in 1989 his best selling what is illustrated children s book series on character education and difficult subjects for children is popular teaching guide for parents teachers and child life professionals boritzer gained national critical acclaim after what is god was published in 1989 although book has caused controversy from religious fundamentalists for its universalist views other current books in what is series include what is love what is death what is beautiful what is funny what is right what is peace what is money what is dreaming what is friend what is true what is family what is feeling series is now also translated into 15 languages boritzer was first published in 1963 at age of 13 when he wrote essay in his english class at wade junior high school in bronx new york on assassination of john f kennedy his essay was included in special anthology by new york city public school children compiled and published by new york city department of education boritzer now lives in venice california and maintains his publishing office there also he has helped numerous other authors to get published through how to get your book published programs boritzer is also yoga teacher who teaches regular classes locally and guest teaches nationally he is also recognized nationally as erudite speaker on teachings of buddha etan boritzer\n",
      "210 2.1507556722888816 all in family is 1975 hong kong adult comedy film directed by mu zhu and produced under golden harvest productions despite starring in film jackie chan does not appear until 1 hour into movie all in family film\n",
      "hard_negative_context:  [['Ian Barry (director)', [['Ian Barry is an Australian director of film and TV.']]], ['Peter Levin', [['Peter Levin is an American director of film, television and theatre.']]]]\n",
      "Where was the director of film Ronnie Rocket born?\n",
      "105 3.4045199174779452 jason moore born october 22 1970 is american director of film theatre and television jason moore director\n",
      "172 1.1025978352085155 jesse edward hobson may 2 1911 november 5 1970 was director of sri international from 1947 to 1955 prior to sri he was director of armour research foundation jesse e hobson\n",
      "417 2.158113883008419 brian patrick kennedy born 5 november 1961 is irish born art museum director who has worked in ireland and australia and now lives and works in united states he is currently director of peabody essex museum he was director of toledo museum of art in ohio from 2010 to 2019 he was director of hood museum of art from 2005 to 2010 and national gallery of australia canberra from 1997 2004 brian kennedy gallery director\n",
      "163 3.3585685828003182 dana blankstein cohen born march 3 1981 is director of israeli academy of film and television she is film director and israeli culture entrepreneur dana blankstein\n",
      "154 1.1195228609334393 s n mathur was director of indian intelligence bureau between september 1975 and february 1980 he was also director general of police in punjab s n mathur\n",
      "350 2.1825741858350556 olav aaraas born 10 july 1950 is norwegian historian and museum director he was born in fredrikstad from 1982 to 1993 he was director of sogn folk museum from 1993 to 2010 he was director of maihaugen and from 2001 he has been director of norwegian museum of cultural history in 2010 he was decorated with royal norwegian order of st olav olav aaraas\n",
      "66 2.3651483716701107 ian barry is australian director of film and tv ian barry director\n",
      "75 2.3380617018914065 peter levin is american director of film television and theatre peter levin\n",
      "hard_negative_context:  [['Jason Moore (director)', [['Jason Moore( born October 22, 1970) is an American director of film, theatre and television.']]], ['Dana Blankstein', [['Dana Blankstein- Cohen( born March 3, 1981) is the director of the Israeli Academy of Film and Television.', 'She is a film director, and an Israeli culture entrepreneur.']]]]\n"
     ]
    }
   ],
   "source": [
    "# run retrieve_hard_negative_context on dev_test.json\n",
    "data = pd.read_json(queries_path)\n",
    "for i,query in data.iterrows():\n",
    "    context = query['context']\n",
    "    evidence = query['evidences']\n",
    "    question = query['question']\n",
    "    hard_negative_context_k2 = retrieve_hard_negative_context(context, evidence, question, k=2)\n",
    "    hard_negative_context_k1 = hard_negative_context_k2[0]\n",
    "    #golden_context = retrieve_golden_context(context, evidence)\n",
    "    print(\"hard_negative_context: \", hard_negative_context_k2)\n",
    "    #print(\"golden_context: \", golden_context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
