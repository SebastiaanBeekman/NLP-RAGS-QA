{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import regex\n",
    "import string\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Retrieval variables\n",
    "queries_path = os.getenv(\"QUERIES_PATH\")\n",
    "\n",
    "# LLM variables\n",
    "llm_id = os.getenv(\"LLM_ID\")\n",
    "num_docs = int(os.getenv(\"TOP_K\"))\n",
    "llm_response_dir = os.getenv(\"LLM_RESPONSE_DIR\")\n",
    "\n",
    "# Evaluation variables\n",
    "evaluation_dir = os.getenv(\"EVALUATION_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory:  ../data/llm_responses/Llama-2-7b-chat-hf/1_doc\n"
     ]
    }
   ],
   "source": [
    "llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "directory = f\"{llm_response_dir}{llm_folder}/{num_docs}_doc\"\n",
    "evaluation_directory = os.path.join(evaluation_dir, f\"{llm_folder}\")\n",
    "print(\"Directory: \", directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def extract_number_from_filename(filename: str, pattern: re.Pattern) -> int:\n",
    "    match = pattern.search(filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def convert_tensors(cell):\n",
    "    if isinstance(cell, list):\n",
    "        return [[t.tolist() if torch.is_tensor(t) else t for t in inner_list] for inner_list in cell]\n",
    "    return cell\n",
    "\n",
    "def read_json(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json(data, file_path: str):\n",
    "    with open(file_path, \"w\") as writer:\n",
    "        json.dump(data, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_files(directory: str, filename_prefix: str) -> pd.DataFrame:\n",
    "    \"\"\" Loads and concatenates data from all pickle files in the directory with the given prefix. \"\"\"\n",
    "    pattern = re.compile(r'(\\d+).pkl')\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.pkl') and filename_prefix in f]\n",
    "    files.sort(key=lambda f: extract_number_from_filename(f, pattern))\n",
    "    print(f\"Files: {files}\")\n",
    "\n",
    "    data_list = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data_list.extend(data)\n",
    "    \n",
    "    data_df = pd.DataFrame(data_list)\n",
    "    data_df['document_indices'] = data_df['document_indices'].apply(convert_tensors)\n",
    "\n",
    "    if 'prompt_tokens_len' in data_df.columns:\n",
    "        data_df['prompt_tokens_len'] = data_df['prompt_tokens_len'].apply(lambda x: x.tolist())\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_json(data_df: pd.DataFrame, directory: str, filename_prefix: str):\n",
    "    \"\"\" Saves the given DataFrame to a JSON file. \"\"\"\n",
    "    data_path = os.path.join(directory, f'{filename_prefix}all.json')\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(data_path):\n",
    "        overwrite = input(f\"File {data_path} already exists. Overwrite? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"No overwrite.\")\n",
    "\n",
    "            results_df = pd.read_json(f'{directory}/{filename_prefix}all_extended.json')\n",
    "            accuracy = round(results_df['ans_match_after_norm'].sum() / len(results_df), 4)\n",
    "            print(\"ACCURACY: \", accuracy)\n",
    "            return None\n",
    "        \n",
    "    data_df.to_json(data_path, orient='records')\n",
    "    return data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ['numdoc1_info_10.pkl', 'numdoc1_info_20.pkl', 'numdoc1_info_30.pkl', 'numdoc1_info_40.pkl', 'numdoc1_info_50.pkl', 'numdoc1_info_60.pkl', 'numdoc1_info_70.pkl', 'numdoc1_info_80.pkl', 'numdoc1_info_90.pkl', 'numdoc1_info_100.pkl', 'numdoc1_info_110.pkl', 'numdoc1_info_120.pkl', 'numdoc1_info_130.pkl', 'numdoc1_info_140.pkl', 'numdoc1_info_150.pkl', 'numdoc1_info_160.pkl', 'numdoc1_info_170.pkl', 'numdoc1_info_180.pkl', 'numdoc1_info_190.pkl', 'numdoc1_info_200.pkl', 'numdoc1_info_210.pkl', 'numdoc1_info_220.pkl', 'numdoc1_info_230.pkl', 'numdoc1_info_240.pkl', 'numdoc1_info_250.pkl', 'numdoc1_info_260.pkl', 'numdoc1_info_270.pkl', 'numdoc1_info_280.pkl', 'numdoc1_info_290.pkl', 'numdoc1_info_300.pkl', 'numdoc1_info_310.pkl', 'numdoc1_info_320.pkl', 'numdoc1_info_330.pkl', 'numdoc1_info_340.pkl', 'numdoc1_info_350.pkl', 'numdoc1_info_360.pkl', 'numdoc1_info_370.pkl', 'numdoc1_info_380.pkl', 'numdoc1_info_390.pkl', 'numdoc1_info_400.pkl', 'numdoc1_info_410.pkl', 'numdoc1_info_420.pkl', 'numdoc1_info_430.pkl', 'numdoc1_info_440.pkl', 'numdoc1_info_450.pkl', 'numdoc1_info_460.pkl', 'numdoc1_info_470.pkl', 'numdoc1_info_480.pkl', 'numdoc1_info_490.pkl', 'numdoc1_info_500.pkl', 'numdoc1_info_510.pkl', 'numdoc1_info_520.pkl', 'numdoc1_info_530.pkl', 'numdoc1_info_540.pkl', 'numdoc1_info_550.pkl', 'numdoc1_info_560.pkl', 'numdoc1_info_570.pkl', 'numdoc1_info_580.pkl', 'numdoc1_info_590.pkl', 'numdoc1_info_600.pkl', 'numdoc1_info_610.pkl', 'numdoc1_info_620.pkl', 'numdoc1_info_630.pkl', 'numdoc1_info_640.pkl', 'numdoc1_info_650.pkl', 'numdoc1_info_660.pkl', 'numdoc1_info_670.pkl', 'numdoc1_info_680.pkl', 'numdoc1_info_690.pkl', 'numdoc1_info_700.pkl', 'numdoc1_info_710.pkl', 'numdoc1_info_720.pkl', 'numdoc1_info_730.pkl', 'numdoc1_info_740.pkl', 'numdoc1_info_750.pkl', 'numdoc1_info_760.pkl', 'numdoc1_info_770.pkl', 'numdoc1_info_780.pkl', 'numdoc1_info_790.pkl', 'numdoc1_info_800.pkl', 'numdoc1_info_810.pkl', 'numdoc1_info_820.pkl', 'numdoc1_info_830.pkl', 'numdoc1_info_840.pkl', 'numdoc1_info_850.pkl', 'numdoc1_info_860.pkl', 'numdoc1_info_870.pkl', 'numdoc1_info_880.pkl', 'numdoc1_info_890.pkl', 'numdoc1_info_900.pkl', 'numdoc1_info_910.pkl', 'numdoc1_info_920.pkl', 'numdoc1_info_930.pkl', 'numdoc1_info_940.pkl', 'numdoc1_info_950.pkl', 'numdoc1_info_960.pkl', 'numdoc1_info_970.pkl', 'numdoc1_info_980.pkl', 'numdoc1_info_990.pkl', 'numdoc1_info_1000.pkl', 'numdoc1_info_1010.pkl', 'numdoc1_info_1020.pkl', 'numdoc1_info_1030.pkl', 'numdoc1_info_1040.pkl', 'numdoc1_info_1050.pkl', 'numdoc1_info_1060.pkl', 'numdoc1_info_1070.pkl', 'numdoc1_info_1080.pkl', 'numdoc1_info_1090.pkl', 'numdoc1_info_1100.pkl', 'numdoc1_info_1110.pkl', 'numdoc1_info_1120.pkl', 'numdoc1_info_1130.pkl', 'numdoc1_info_1140.pkl', 'numdoc1_info_1150.pkl', 'numdoc1_info_1160.pkl', 'numdoc1_info_1170.pkl', 'numdoc1_info_1180.pkl', 'numdoc1_info_1190.pkl', 'numdoc1_info_1200.pkl']\n",
      "Data saved to ../data/llm_responses/Llama-2-7b-chat-hf/1_doc/numdoc1_info_all.json.\n",
      "Data shape:  (1200, 6)\n",
      "Data columns:  Index(['id', 'query', 'prompt', 'document_indices', 'prompt_tokens_len',\n",
      "       'generated_answer'],\n",
      "      dtype='object')\n",
      "Data sample:                                     id  \\\n",
      "0  [8813f87c0bdd11eba7f7acde48001122]   \n",
      "1  [61a46987092f11ebbdaeac1f6bf848b6]   \n",
      "2  [e2a3bf2a0bdd11eba7f7acde48001122]   \n",
      "3  [0cd3bdea0bde11eba7f7acde48001122]   \n",
      "4  [f9dcb4a60bda11eba7f7acde48001122]   \n",
      "\n",
      "                                               query  \\\n",
      "0  [Who is the mother of the director of film Pol...   \n",
      "1  [Which film came out first, Blind Shaft or The...   \n",
      "2  [When did John V, Prince Of Anhalt-Zerbst's fa...   \n",
      "3  [What is the award that the director of film W...   \n",
      "4  [Where was the director of film Ronnie Rocket ...   \n",
      "\n",
      "                                              prompt document_indices  \\\n",
      "0  [You are given a question and you MUST respond...       [[427816]]   \n",
      "1  [You are given a question and you MUST respond...       [[458648]]   \n",
      "2  [You are given a question and you MUST respond...       [[273667]]   \n",
      "3  [You are given a question and you MUST respond...       [[171055]]   \n",
      "4  [You are given a question and you MUST respond...       [[167958]]   \n",
      "\n",
      "  prompt_tokens_len                                   generated_answer  \n",
      "0             [598]  [NO-RES. The information about the mother of t...  \n",
      "1             [230]                    [THE MASK OF FU MANCHU\\nNO-RES]  \n",
      "2             [248]                                     [1516\\nNO-RES]  \n",
      "3             [212]                 [NO-RES\\nDocument [234867](Title:]  \n",
      "4             [435]           [ROCKET\\nExplanation: In document [1679]  \n"
     ]
    }
   ],
   "source": [
    "filename_prefix = f\"numdoc{num_docs}_info_\"\n",
    "data_df = load_pickle_files(directory, filename_prefix)\n",
    "data_path = save_data_to_json(data_df, directory, filename_prefix)\n",
    "\n",
    "if data_path:\n",
    "    print(f\"Data saved to {data_path}.\")\n",
    "    print(\"Data shape: \", data_df.shape)\n",
    "    print(\"Data columns: \", data_df.columns)\n",
    "    print(\"Data sample: \", data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(queries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Małgorzata Braunek'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['_id'] == \"8813f87c0bdd11eba7f7acde48001122\"][\"answer\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted from chemdataextractor.text.normalize\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tools for normalizing text.\n",
    "https://github.com/mcs07/ChemDataExtractor\n",
    ":copyright: Copyright 2016 by Matt Swain.\n",
    ":license: MIT\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "'Software'), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
    "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "#: Control characters.\n",
    "CONTROLS = {\n",
    "    '\\u0001', '\\u0002', '\\u0003', '\\u0004', '\\u0005', '\\u0006', '\\u0007', '\\u0008', '\\u000e', '\\u000f', '\\u0011',\n",
    "    '\\u0012', '\\u0013', '\\u0014', '\\u0015', '\\u0016', '\\u0017', '\\u0018', '\\u0019', '\\u001a', '\\u001b',\n",
    "}\n",
    "# There are further control characters, but they are instead replaced with a space by unicode normalization\n",
    "# '\\u0009', '\\u000a', '\\u000b', '\\u000c', '\\u000d', '\\u001c',  '\\u001d', '\\u001e', '\\u001f'\n",
    "\n",
    "\n",
    "#: Hyphen and dash characters.\n",
    "HYPHENS = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '‐',  # \\u2010 Hyphen\n",
    "    '‑',  # \\u2011 Non-breaking hyphen\n",
    "    '⁃',  # \\u2043 Hyphen bullet\n",
    "    '‒',  # \\u2012 figure dash\n",
    "    '–',  # \\u2013 en dash\n",
    "    '—',  # \\u2014 em dash\n",
    "    '―',  # \\u2015 horizontal bar\n",
    "}\n",
    "\n",
    "#: Minus characters.\n",
    "MINUSES = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '−',  # \\u2212 Minus\n",
    "    '－',  # \\uff0d Full-width Hyphen-minus\n",
    "    '⁻',  # \\u207b Superscript minus\n",
    "}\n",
    "\n",
    "#: Plus characters.\n",
    "PLUSES = {\n",
    "    '+',  # \\u002b Plus\n",
    "    '＋',  # \\uff0b Full-width Plus\n",
    "    '⁺',  # \\u207a Superscript plus\n",
    "}\n",
    "\n",
    "#: Slash characters.\n",
    "SLASHES = {\n",
    "    '/',  # \\u002f Solidus\n",
    "    '⁄',  # \\u2044 Fraction slash\n",
    "    '∕',  # \\u2215 Division slash\n",
    "}\n",
    "\n",
    "#: Tilde characters.\n",
    "TILDES = {\n",
    "    '~',  # \\u007e Tilde\n",
    "    '˜',  # \\u02dc Small tilde\n",
    "    '⁓',  # \\u2053 Swung dash\n",
    "    '∼',  # \\u223c Tilde operator #in mbert vocab\n",
    "    '∽',  # \\u223d Reversed tilde\n",
    "    '∿',  # \\u223f Sine wave\n",
    "    '〜',  # \\u301c Wave dash #in mbert vocab\n",
    "    '～',  # \\uff5e Full-width tilde #in mbert vocab\n",
    "}\n",
    "\n",
    "#: Apostrophe characters.\n",
    "APOSTROPHES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '’',  # \\u2019\n",
    "    '՚',  # \\u055a\n",
    "    'Ꞌ',  # \\ua78b\n",
    "    'ꞌ',  # \\ua78c\n",
    "    '＇',  # \\uff07\n",
    "}\n",
    "\n",
    "#: Single quote characters.\n",
    "SINGLE_QUOTES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '‘',  # \\u2018\n",
    "    '’',  # \\u2019\n",
    "    '‚',  # \\u201a\n",
    "    '‛',  # \\u201b\n",
    "\n",
    "}\n",
    "\n",
    "#: Double quote characters.\n",
    "DOUBLE_QUOTES = {\n",
    "    '\"',  # \\u0022\n",
    "    '“',  # \\u201c\n",
    "    '”',  # \\u201d\n",
    "    '„',  # \\u201e\n",
    "    '‟',  # \\u201f\n",
    "}\n",
    "\n",
    "#: Accent characters.\n",
    "ACCENTS = {\n",
    "    '`',  # \\u0060\n",
    "    '´',  # \\u00b4\n",
    "}\n",
    "\n",
    "#: Prime characters.\n",
    "PRIMES = {\n",
    "    '′',  # \\u2032\n",
    "    '″',  # \\u2033\n",
    "    '‴',  # \\u2034\n",
    "    '‵',  # \\u2035\n",
    "    '‶',  # \\u2036\n",
    "    '‷',  # \\u2037\n",
    "    '⁗',  # \\u2057\n",
    "}\n",
    "\n",
    "#: Quote characters, including apostrophes, single quotes, double quotes, accents and primes.\n",
    "QUOTES = APOSTROPHES | SINGLE_QUOTES | DOUBLE_QUOTES | ACCENTS | PRIMES\n",
    "\n",
    "def normalize(text):\n",
    "    for control in CONTROLS:\n",
    "        text = text.replace(control, '')\n",
    "    text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n",
    "\n",
    "    for hyphen in HYPHENS | MINUSES:\n",
    "        text = text.replace(hyphen, '-')\n",
    "    text = text.replace('\\u00ad', '')\n",
    "\n",
    "    for double_quote in DOUBLE_QUOTES:\n",
    "        text = text.replace(double_quote, '\"')  # \\u0022\n",
    "    for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n",
    "        text = text.replace(single_quote, \"'\")  # \\u0027\n",
    "    text = text.replace('′', \"'\")     # \\u2032 prime\n",
    "    text = text.replace('‵', \"'\")     # \\u2035 reversed prime\n",
    "    text = text.replace('″', \"''\")    # \\u2033 double prime\n",
    "    text = text.replace('‶', \"''\")    # \\u2036 reversed double prime\n",
    "    text = text.replace('‴', \"'''\")   # \\u2034 triple prime\n",
    "    text = text.replace('‷', \"'''\")   # \\u2037 reversed triple prime\n",
    "    text = text.replace('⁗', \"''''\")  # \\u2057 quadruple prime\n",
    "\n",
    "    text = text.replace('…', '...').replace(' . . . ', ' ... ')  # \\u2026\n",
    "\n",
    "    for slash in SLASHES:\n",
    "        text = text.replace(slash, '/')\n",
    "\n",
    "    for tilde in TILDES:\n",
    "       text = text.replace(tilde, '~')\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization adapted from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def remove_articles(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes articles ('a', 'an', 'the') from the text.\n",
    "    \"\"\"\n",
    "    return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "def white_space_fix(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes extra whitespace in the text by collapsing multiple spaces into one.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_punc(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the text and replaces it with a space.\n",
    "    \"\"\"\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' ')\n",
    "    return text\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts all characters in the text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_answer(s: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes answers by removing articles, punctuation, fixing whitespace, and optionally converting to lowercase.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        s = lower(s)\n",
    "    s = normalize(s)\n",
    "    return white_space_fix(remove_articles(remove_punc(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_answers_matching(prediction: str, ground_truth: str) -> float:\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "    if normalized_ground_truth in normalized_prediction:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answer_in_text(text: str, answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if any of the provided answers are present in the given text after normalization.\n",
    "    \"\"\"\n",
    "    normalized_answer_lower = normalize_answer(answer, lowercase=True)\n",
    "    normalized_answer = normalize_answer(answer, lowercase=False)\n",
    "    normalized_text = white_space_fix(remove_punc(text))\n",
    "\n",
    "    if (answer in text or \n",
    "        normalized_answer_lower in normalized_text or \n",
    "        normalized_answer in normalized_text):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_generation_results(file_path: str, df: pd.DataFrame) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        file_data = json.load(fin)\n",
    "\n",
    "        for example in file_data:\n",
    "            example_ids = example['id']\n",
    "            queries = example['query']\n",
    "            prompts = example['prompt']\n",
    "            document_indices = list(zip(*example['document_indices']))\n",
    "            prompt_tokens_lens = example['prompt_tokens_len']\n",
    "            generated_answers = example['generated_answer']\n",
    "\n",
    "            for i in range(len(example_ids)):\n",
    "                example_id = example_ids[i]\n",
    "                query = queries[i]\n",
    "                documents_idx = list(document_indices[i])\n",
    "                generated_answer = generated_answers[i]\n",
    "                prompt = prompts[i]\n",
    "                prompt_tokens_len = prompt_tokens_lens[i]\n",
    "\n",
    "                answer = df[df['_id'] == str(example_id)][\"answer\"].values[0]\n",
    "\n",
    "                ans_match_after_norm: bool = are_answers_matching(generated_answer, answer)\n",
    "                ans_in_documents: bool = is_answer_in_text(prompt, answer)\n",
    "                data.append({\n",
    "                    'example_id': str(example_id),\n",
    "                    'query': query,\n",
    "                    'prompt': prompt,\n",
    "                    'document_indices': documents_idx,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'answer': answer,\n",
    "                    'ans_match_after_norm': ans_match_after_norm,\n",
    "                    'ans_in_documents': ans_in_documents,\n",
    "                    \"prompt_tokens_len\": prompt_tokens_len,\n",
    "                })\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = read_generation_results(data_path, df) # id, query, prompt, document_indices, prompt_tokens_len, generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.0942\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "accuracy = round(results_df['ans_match_after_norm'].sum() / len(results_df), 4)\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "os.makedirs(evaluation_directory, exist_ok=True)\n",
    "results_df.to_json(os.path.join(evaluation_directory, f'{filename_prefix}all_extended.json'), orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
