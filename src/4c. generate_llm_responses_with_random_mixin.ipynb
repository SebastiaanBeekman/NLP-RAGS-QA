{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries by LLM; Are not installed per default\n",
    "# ! pip install accelerate\n",
    "# ! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, StoppingCriteriaList, StoppingCriteria,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Embeddings variables\n",
    "corpus_path = os.getenv(\"CORPUS_PATH\")\n",
    "\n",
    "# Retrieval variables\n",
    "queries_path = os.getenv(\"QUERIES_PATH\")\n",
    "\n",
    "## LLM variables\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "llm_id = os.getenv(\"LLM_ID\")\n",
    "num_docs = int(os.getenv(\"TOP_K\"))\n",
    "max_input_length = int(os.getenv(\"MAX_INPUT_LENGTH\"))\n",
    "max_output_length = int(os.getenv(\"MAX_OUTPUT_LENGTH\"))\n",
    "normalize_queries = os.getenv(\"NORMALIZE_QUERIES\") == \"True\"\n",
    "context_retrieval_dir = os.getenv(\"CONTEXT_RETRIEVAL_DIR\")\n",
    "llm_response_dir = os.getenv(\"LLM_RESPONSE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/martijn/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = pickle.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_pickle(data, file_path: str):\n",
    "    with open(file_path, \"wb\") as writer:\n",
    "        pickle.dump(data, writer)\n",
    "\n",
    "\n",
    "def read_json(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data\n",
    "\n",
    "def read_corpus(corpus_path: str):\n",
    "    new_corpus = []\n",
    "    corpus = read_json(corpus_path).values()\n",
    "    for i, record in enumerate(corpus):\n",
    "        record[\"full_corpus_idx\"] = i\n",
    "        new_corpus.append(record)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Run seeder before proceeding\n",
    "set_seeds(seed)\n",
    "print(f\"Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    \"\"\"\n",
    "    A class for loading and generating text using a Language Model (LM) with support for quantization\n",
    "    and custom stopping criteria.\n",
    "    \n",
    "    Attributes:\n",
    "        model_id (str): Identifier for the model to load.\n",
    "        device (str): Device to run the model on, e.g. 'cuda'.\n",
    "        quantization_bits (Optional[int]): Number of bits for quantization, supports 4 or 8 bits.\n",
    "        stop_list (Optional[List[str]]): List of tokens where generation should stop.\n",
    "        model_max_length (int): Maximum length of the model inputs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str, \n",
    "        device: str = 'cuda', \n",
    "        quantization_bits: Optional[int] = None, \n",
    "        stop_list: Optional[List[str]] = None, \n",
    "        model_max_length: int = 4096\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        self.stop_list = stop_list\n",
    "        if stop_list is None:\n",
    "            self.stop_list = ['\\nHuman:', '\\n```\\n', '\\nQuestion:', '<|endoftext|>', '\\n']\n",
    "        \n",
    "        self.bnb_config = self._set_quantization(quantization_bits)\n",
    "        self.model, self.tokenizer = self._initialize_model_tokenizer(model_id)\n",
    "        self.stopping_criteria = self._define_stopping_criteria()\n",
    "        \n",
    "\n",
    "    def _set_quantization(self, quantization_bits: Optional[int]) -> Optional[BitsAndBytesConfig]:\n",
    "        \"\"\"\n",
    "        Configure quantization settings based on the specified number of bits.\n",
    "        \"\"\"\n",
    "        if quantization_bits in [4, 8]:\n",
    "            bnb_config = BitsAndBytesConfig()\n",
    "            if quantization_bits == 4:\n",
    "                bnb_config.load_in_4bit = True\n",
    "                bnb_config.bnb_4bit_quant_type = 'nf4'\n",
    "                bnb_config.bnb_4bit_use_double_quant = True\n",
    "                bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            elif quantization_bits == 8:\n",
    "                bnb_config.load_in_8bit = True\n",
    "            return bnb_config\n",
    "        return None\n",
    " \n",
    "\n",
    "    def _initialize_model_tokenizer(self, model_id: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "        \"\"\"\n",
    "        Initializes the model and tokenizer with the given model ID.\n",
    "        \"\"\"\n",
    "        model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_config.max_seq_len = self.model_max_length\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            quantization_config=self.bnb_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map='auto',\n",
    "        )\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, padding_side=\"left\", truncation_side=\"left\",\n",
    "            model_max_length=self.model_max_length\n",
    "        )\n",
    "        # Most LLMs don't have a pad token by default\n",
    "        tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "\n",
    "    def _define_stopping_criteria(self) -> StoppingCriteriaList:\n",
    "        \"\"\"\n",
    "        Defines stopping criteria for text generation based on the provided stop_list.\n",
    "        \"\"\"\n",
    "        stop_token_ids = [self.tokenizer(x)['input_ids'] for x in self.stop_list]\n",
    "        stop_token_ids = [torch.LongTensor(x).to(self.device) for x in stop_token_ids]\n",
    "\n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "                for stop_ids in stop_token_ids:\n",
    "                    if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        return StoppingCriteriaList([StopOnTokens()])\n",
    "    \n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 15) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt for generation.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: The generated text responses.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self.model_max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.1,\n",
    "            stopping_criteria=self.stopping_criteria,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8963c2a0c6c4be1bd1fb611460b2ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_input_length\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_id, device, quantization_bits, stop_list, model_max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_quantization(quantization_bits)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_model_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_stopping_criteria()\n",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m, in \u001b[0;36mLLM._initialize_model_tokenizer\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m     54\u001b[0m model_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m model_config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[0;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     67\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     68\u001b[0m     model_id, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m~/Coding/TUDelft/cs4360-nlp-project/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/Coding/TUDelft/cs4360-nlp-project/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3202\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3202\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3205\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3206\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/Coding/TUDelft/cs4360-nlp-project/.venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "        llm_id, device, quantization_bits=4, \n",
    "        model_max_length=max_input_length\n",
    "    )\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 563424 records and 1200 search results for Top-5.\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[(['427816'], array([0.9740324], dtype=float32)), (['458648'], array([1.0431149], dtype=float32)), (['273667'], array([1.2163849], dtype=float32)), (['171055'], array([0.98047715], dtype=float32)), (['167958'], array([1.1058712], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(corpus_path)\n",
    "search_results = read_pickle(context_retrieval_dir)\n",
    "print(f\"Loaded {len(corpus)} records and {len(search_results)} search results for Top-{num_docs}.\")\n",
    "print(type(corpus))\n",
    "print(type(search_results))\n",
    "print(search_results[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted from chemdataextractor.text.normalize\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tools for normalizing text.\n",
    "https://github.com/mcs07/ChemDataExtractor\n",
    ":copyright: Copyright 2016 by Matt Swain.\n",
    ":license: MIT\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "'Software'), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
    "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "#: Control characters.\n",
    "CONTROLS = {\n",
    "    '\\u0001', '\\u0002', '\\u0003', '\\u0004', '\\u0005', '\\u0006', '\\u0007', '\\u0008', '\\u000e', '\\u000f', '\\u0011',\n",
    "    '\\u0012', '\\u0013', '\\u0014', '\\u0015', '\\u0016', '\\u0017', '\\u0018', '\\u0019', '\\u001a', '\\u001b',\n",
    "}\n",
    "# There are further control characters, but they are instead replaced with a space by unicode normalization\n",
    "# '\\u0009', '\\u000a', '\\u000b', '\\u000c', '\\u000d', '\\u001c',  '\\u001d', '\\u001e', '\\u001f'\n",
    "\n",
    "\n",
    "#: Hyphen and dash characters.\n",
    "HYPHENS = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '‐',  # \\u2010 Hyphen\n",
    "    '‑',  # \\u2011 Non-breaking hyphen\n",
    "    '⁃',  # \\u2043 Hyphen bullet\n",
    "    '‒',  # \\u2012 figure dash\n",
    "    '–',  # \\u2013 en dash\n",
    "    '—',  # \\u2014 em dash\n",
    "    '―',  # \\u2015 horizontal bar\n",
    "}\n",
    "\n",
    "#: Minus characters.\n",
    "MINUSES = {\n",
    "    '-',  # \\u002d Hyphen-minus\n",
    "    '−',  # \\u2212 Minus\n",
    "    '－',  # \\uff0d Full-width Hyphen-minus\n",
    "    '⁻',  # \\u207b Superscript minus\n",
    "}\n",
    "\n",
    "#: Plus characters.\n",
    "PLUSES = {\n",
    "    '+',  # \\u002b Plus\n",
    "    '＋',  # \\uff0b Full-width Plus\n",
    "    '⁺',  # \\u207a Superscript plus\n",
    "}\n",
    "\n",
    "#: Slash characters.\n",
    "SLASHES = {\n",
    "    '/',  # \\u002f Solidus\n",
    "    '⁄',  # \\u2044 Fraction slash\n",
    "    '∕',  # \\u2215 Division slash\n",
    "}\n",
    "\n",
    "#: Tilde characters.\n",
    "TILDES = {\n",
    "    '~',  # \\u007e Tilde\n",
    "    '˜',  # \\u02dc Small tilde\n",
    "    '⁓',  # \\u2053 Swung dash\n",
    "    '∼',  # \\u223c Tilde operator #in mbert vocab\n",
    "    '∽',  # \\u223d Reversed tilde\n",
    "    '∿',  # \\u223f Sine wave\n",
    "    '〜',  # \\u301c Wave dash #in mbert vocab\n",
    "    '～',  # \\uff5e Full-width tilde #in mbert vocab\n",
    "}\n",
    "\n",
    "#: Apostrophe characters.\n",
    "APOSTROPHES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '’',  # \\u2019\n",
    "    '՚',  # \\u055a\n",
    "    'Ꞌ',  # \\ua78b\n",
    "    'ꞌ',  # \\ua78c\n",
    "    '＇',  # \\uff07\n",
    "}\n",
    "\n",
    "#: Single quote characters.\n",
    "SINGLE_QUOTES = {\n",
    "    \"'\",  # \\u0027\n",
    "    '‘',  # \\u2018\n",
    "    '’',  # \\u2019\n",
    "    '‚',  # \\u201a\n",
    "    '‛',  # \\u201b\n",
    "\n",
    "}\n",
    "\n",
    "#: Double quote characters.\n",
    "DOUBLE_QUOTES = {\n",
    "    '\"',  # \\u0022\n",
    "    '“',  # \\u201c\n",
    "    '”',  # \\u201d\n",
    "    '„',  # \\u201e\n",
    "    '‟',  # \\u201f\n",
    "}\n",
    "\n",
    "#: Accent characters.\n",
    "ACCENTS = {\n",
    "    '`',  # \\u0060\n",
    "    '´',  # \\u00b4\n",
    "}\n",
    "\n",
    "#: Prime characters.\n",
    "PRIMES = {\n",
    "    '′',  # \\u2032\n",
    "    '″',  # \\u2033\n",
    "    '‴',  # \\u2034\n",
    "    '‵',  # \\u2035\n",
    "    '‶',  # \\u2036\n",
    "    '‷',  # \\u2037\n",
    "    '⁗',  # \\u2057\n",
    "}\n",
    "\n",
    "#: Quote characters, including apostrophes, single quotes, double quotes, accents and primes.\n",
    "QUOTES = APOSTROPHES | SINGLE_QUOTES | DOUBLE_QUOTES | ACCENTS | PRIMES\n",
    "\n",
    "def normalize(text):\n",
    "    for control in CONTROLS:\n",
    "        text = text.replace(control, '')\n",
    "    text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n",
    "\n",
    "    for hyphen in HYPHENS | MINUSES:\n",
    "        text = text.replace(hyphen, '-')\n",
    "    text = text.replace('\\u00ad', '')\n",
    "\n",
    "    for double_quote in DOUBLE_QUOTES:\n",
    "        text = text.replace(double_quote, '\"')  # \\u0022\n",
    "    for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n",
    "        text = text.replace(single_quote, \"'\")  # \\u0027\n",
    "    text = text.replace('′', \"'\")     # \\u2032 prime\n",
    "    text = text.replace('‵', \"'\")     # \\u2035 reversed prime\n",
    "    text = text.replace('″', \"''\")    # \\u2033 double prime\n",
    "    text = text.replace('‶', \"''\")    # \\u2036 reversed double prime\n",
    "    text = text.replace('‴', \"'''\")   # \\u2034 triple prime\n",
    "    text = text.replace('‷', \"'''\")   # \\u2037 reversed triple prime\n",
    "    text = text.replace('⁗', \"''''\")  # \\u2057 quadruple prime\n",
    "\n",
    "    text = text.replace('…', '...').replace(' . . . ', ' ... ')  # \\u2026\n",
    "\n",
    "    for slash in SLASHES:\n",
    "        text = text.replace(slash, '/')\n",
    "\n",
    "    for tilde in TILDES:\n",
    "       text = text.replace(tilde, '~')\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_document(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SHA-256 hash for a given text.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        corpus: List[Dict],  \n",
    "        tokenizer: AutoTokenizer,\n",
    "        search_results: List[Tuple[List[int], List[float]]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.search_results = search_results\n",
    "        self.data_path = queries_path\n",
    "        self.max_tokenized_length = max_input_length - 2\n",
    "        self.do_normalize_query = normalize_queries\n",
    "        self.num_documents_in_context = num_docs\n",
    "    \n",
    "        \n",
    "        self._validate_initialization_parameters()\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "    def _validate_initialization_parameters(self):\n",
    "        if self.num_documents_in_context <= 0:\n",
    "            raise ValueError(\"num_documents_in_context must be positive.\")\n",
    "        \n",
    "        if self.max_tokenized_length <= 0:\n",
    "            raise ValueError(\"max_tokenized_length must be positive.\")\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        try:\n",
    "            with open(self.data_path, \"r\") as reader:\n",
    "                data = json.load(reader)\n",
    "            self.process_file_data(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {self.data_path}: {e}\")\n",
    "\n",
    "\n",
    "    def process_file_data(self, data: List[Dict]):  \n",
    "        \"\"\"\n",
    "        Processes each example in the dataset to prepare prompts for the LLM.\n",
    "\n",
    "        This involves assembling document contexts, normalizing text as needed,\n",
    "        and checking against the maximum token length to ensure compatibility with the LLM's input specifications.\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): The dataset, where each entry contains information about an example,\n",
    "            including the example's ID, the gold document index, answers, and the query.\n",
    "        \"\"\"\n",
    "        self.ids = []\n",
    "        self.queries = []\n",
    "        self.prompts = []\n",
    "        self.excluded_samples_ids = []\n",
    "        self.preprocessed_data = []\n",
    "        self.prompt_tokens_lengths = []\n",
    "\n",
    "        for i, sample in enumerate(data):\n",
    "            id = sample['_id']\n",
    "            query = sample['question']\n",
    "\n",
    "            # Retrieve the top-k documents for the query\n",
    "            formatted_documents, document_indices = self.prepare_documents_for_prompt(i)\n",
    "\n",
    "            # Normalize the query & build the prompt\n",
    "            documents_str = '\\n'.join(formatted_documents)\n",
    "            if self.do_normalize_query:\n",
    "                query = normalize(query)\n",
    "            prompt = self.build_qa_prompt(query, documents_str)\n",
    "\n",
    "            # Check if the prompt exceeds 'max_tokenized_length'\n",
    "            tokens = self.tokenizer.tokenize(prompt)\n",
    "            tokens_len = len(tokens)\n",
    "            if tokens_len >= self.max_tokenized_length:\n",
    "                self.excluded_samples_ids.append((i, id))\n",
    "                print(\"Skipping example {} due to prompt length.\".format((i, id)))\n",
    "                continue\n",
    "\n",
    "            if len(formatted_documents) != self.num_documents_in_context:\n",
    "                print(f\"Warning: Not enough documents for example {i}.\")\n",
    "\n",
    "            # If the prompt is valid, add it to the dataset\n",
    "            self.preprocessed_data.append((formatted_documents, list(document_indices)))\n",
    "            self.ids.append(id)\n",
    "            self.queries.append(query)\n",
    "            self.prompts.append(prompt)\n",
    "            self.prompt_tokens_lengths.append(tokens_len)\n",
    "\n",
    "\n",
    "    def prepare_documents_for_prompt(self, i: int) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Returns the prepared documents for prompt with id i, based on the search results for that prompt id.\"\"\"\n",
    "        indices = self._get_indices(i) # Get the indices of the top-k documents\n",
    "        return self._get_documents_from_indices(indices)\n",
    "\n",
    "\n",
    "    def _get_indices(self, idx: int) -> List[int]:\n",
    "        \"\"\"Get the indices of the relevant documents for a given prompt as gathered by the retriever.\"\"\"\n",
    "        indices, _ = self.search_results[idx]\n",
    "        return indices\n",
    "    \n",
    "\n",
    "    def _format_document(index, title, text) -> str:\n",
    "        \"\"\"Format a document in the appropriate manner for prompt generation\"\"\"\n",
    "        return f\"Document [{index}](Title: {title}) {text}\"\n",
    "    \n",
    "\n",
    "    def _get_documents_from_indices(self, indices: List[int]) -> Tuple[List[str], List[int]]:\n",
    "        documents_info = [self.corpus[i] for i in map(int, indices)]\n",
    "        \n",
    "        seen_hashes = set()\n",
    "        # List to store the indices of documents actually added\n",
    "        document_indices = []  \n",
    "        formatted_documents = []\n",
    "        for doc_info in documents_info:\n",
    "            if len(formatted_documents) == self.num_documents_in_context:\n",
    "                break\n",
    "\n",
    "            doc_idx = doc_info['full_corpus_idx']\n",
    "            title = doc_info['title']\n",
    "            text = doc_info['text']\n",
    "\n",
    "            doc_hash = hash_document(text)\n",
    "            # Skip the document if it is a duplicate\n",
    "            if doc_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(doc_hash)\n",
    "\n",
    "            doc_str = self._format_document(doc_idx, title, text)\n",
    "            formatted_documents.append(doc_str)\n",
    "            document_indices.append(doc_idx)\n",
    "\n",
    "        return formatted_documents, document_indices\n",
    "\n",
    "\n",
    "    def build_qa_prompt(self, query: str, documents_str: str) -> str:\n",
    "        task_instruction = \"You are given a question and you MUST respond by EXTRACTING the answer (max 5 tokens) from one of the provided documents. If none of the documents contain the answer, respond with NO-RES.\"\n",
    "        prompt = f\"\"\"{task_instruction}\\nDocuments:\\n{documents_str}\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "\n",
    "        # Custom prompt format for mpt models\n",
    "        if 'mpt' in self.tokenizer.name_or_path:\n",
    "            INSTRUCTION_KEY = \"### Instruction:\"\n",
    "            RESPONSE_KEY = \"### Response:\"\n",
    "            INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\"\"\".format(\n",
    "                intro=INTRO_BLURB,\n",
    "                instruction_key=INSTRUCTION_KEY,\n",
    "                instruction=\"{instruction}\",\n",
    "                response_key=RESPONSE_KEY,\n",
    "            )\n",
    "            prompt = PROMPT_FOR_GENERATION_FORMAT.format(\n",
    "                instruction=prompt[:-8]\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        _, document_indices = self.preprocessed_data[idx]\n",
    "\n",
    "        return {\n",
    "            \"id\": self.ids[idx],\n",
    "            \"query\": self.queries[idx],\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"document_indices\": document_indices,\n",
    "            \"prompt_tokens_len\": self.prompt_tokens_lengths[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset_and_loader(\n",
    "    corpus: List[Dict], \n",
    "    search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, \n",
    "        tokenizer=tokenizer,\n",
    "        search_results=search_results\n",
    "    )\n",
    "    \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataloader = initialize_dataset_and_loader(\n",
    "    corpus, search_results, tokenizer\n",
    ")\n",
    "print(f\"Initialized prompt dataset with {len(prompt_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info():\n",
    "    print(\"INFO:\")\n",
    "    print(f\"DATA: {queries_path}\")\n",
    "    print(f\"MODEL: {llm_id}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {num_docs}\")\n",
    "    print(f\"BATCH SIZE: {batch_size}\")\n",
    "    print(f\"SAVE EVERY: {save_every}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:\n",
      "DATA: ../data/dev_1200.json\n",
      "MODEL: meta-llama/Llama-2-7b-chat-hf\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: 64\n",
      "SAVE EVERY: 500\n"
     ]
    }
   ],
   "source": [
    "print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save(llm: LLM, prompt_dataloader: DataLoader):\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{llm_response_dir}/{llm_folder}/{num_docs}_doc\"\n",
    "    if not os.path.exists(saving_dir):\n",
    "        os.makedirs(saving_dir)\n",
    "\n",
    "    # MPT has a different answer string in the prompt\n",
    "    answer_string_in_prompt = \"### Response:\" if 'mpt' in llm_id else \"Answer:\"\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        generated_output = llm.generate(prompts, max_new_tokens=max_output_length)\n",
    "        \n",
    "        generated_answers = []\n",
    "        for output in generated_output:\n",
    "            start = output.find(answer_string_in_prompt) + len(answer_string_in_prompt)\n",
    "            response = output[start:].strip()\n",
    "            generated_answers.append(response)\n",
    "\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_docs}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save(llm, prompt_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
