{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from transformers import PreTrainedModel, AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# General variables\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "save_every = int(os.getenv(\"SAVE_EVERY\"))\n",
    "\n",
    "# Embeddings variables\n",
    "corpus_path = os.getenv(\"CORPUS_PATH\")\n",
    "max_length_encoder = int(os.getenv(\"MAX_LENGTH_ENCODER\"))\n",
    "normalize_embeddings = os.getenv(\"NORMALIZE_EMBEDDINGS\") == \"True\"\n",
    "lower_case = os.getenv(\"LOWER_CASE\") == \"True\"\n",
    "normalize_text = os.getenv(\"NORMALIZE_TEXT\") == \"True\"\n",
    "embeddings_dir = os.getenv(\"EMBEDDINGS_DIR\")\n",
    "\n",
    "# Indexing variables\n",
    "vector_size = int(os.getenv(\"VECTOR_SIZE\"))\n",
    "faiss_dir = os.getenv(\"FAISS_DIR\")\n",
    "\n",
    "# Retrieval variables\n",
    "top_k = int(os.getenv(\"TOP_K\"))\n",
    "use_gpu = os.getenv(\"USE_GPU\") == \"True\"\n",
    "gpu_ids = [int(gpu_id) for gpu_id in os.getenv(\"GPU_IDS\").split(\",\")]\n",
    "use_test_set = os.getenv(\"USE_TEST_SET\") == \"True\"\n",
    "index_batch_size = int(os.getenv(\"INDEX_BATCH_SIZE\"))\n",
    "search_dir = os.getenv(\"SEARCH_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\" if use_test_set else \"train\"\n",
    "\n",
    "if use_gpu and (gpu_ids is None or len(gpu_ids) == 0):\n",
    "    raise Exception('gpu_ids must be set when se_gpu is used.')\n",
    "\n",
    "split_paths = {\n",
    "    \"train\": {\n",
    "        \"data_path\": '../data/train.json',\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"data_path\": '../data/dev.json',\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"data_path\": '../data/test.json',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as reader:\n",
    "        data = pickle.load(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_pickle(data, file_path: str):\n",
    "    with open(file_path, \"wb\") as writer:\n",
    "        pickle.dump(data, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run seeder before proceeding\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries() -> List[str]:\n",
    "    df = pd.read_json(split_paths[split]['data_path'])\n",
    "    queries = df['query'].tolist() if 'query' in df.columns else df['question'].tolist()\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(object):\n",
    "    def __init__(self, vector_size: int):     \n",
    "        self.index = faiss.IndexFlatIP(vector_size)\n",
    "        self.index_id_to_db_id = []\n",
    "\n",
    "    def search_knn(\n",
    "        self, \n",
    "        query_vectors: np.array, \n",
    "        top_docs: int, \n",
    "        index_batch_size: int = 2048\n",
    "    ) -> List[Tuple[List[str], List[float]]]:\n",
    "        \"\"\"\n",
    "        Performs a k-nearest neighbor search for the given query vectors.\n",
    "\n",
    "        Args:\n",
    "            query_vectors (np.array): A numpy array of query vectors.\n",
    "            top_docs (int): The number of top documents to return for each query.\n",
    "            index_batch_size (int): The batch size to use when indexing.\n",
    "        \n",
    "        Returns:\n",
    "            A list of tuples, each containing a list of document IDs and a list of corresponding scores.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        nbatch = (len(query_vectors)-1) // index_batch_size + 1\n",
    "        for k in range(nbatch):\n",
    "            start_idx = k*index_batch_size\n",
    "            end_idx = min((k+1)*index_batch_size, len(query_vectors))\n",
    "            q = query_vectors[start_idx: end_idx]\n",
    "            scores, indexes = self.index.search(q, top_docs)\n",
    "            # convert to external ids\n",
    "            db_ids = [[str(self.index_id_to_db_id[i]) for i in query_top_idxs] for query_top_idxs in indexes]\n",
    "            result.extend([(db_ids[i], scores[i]) for i in range(len(db_ids))])\n",
    "        return result\n",
    "\n",
    "\n",
    "    def deserialize_from(\n",
    "        self, \n",
    "        dir_path: str, \n",
    "        index_file_name: Optional[str] = None, \n",
    "        meta_file_name: Optional[str] = None,\n",
    "        gpu_id: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads the index and its metadata from disk.\n",
    "\n",
    "        Args:\n",
    "            dir_path (str): The directory path from where to load the index and metadata.\n",
    "            index_file_name (Optional[str]): Optional custom name for the index file.\n",
    "            meta_file_name (Optional[str]): Optional custom name for the metadata file.\n",
    "        \"\"\"\n",
    "        if index_file_name is None:\n",
    "            index_file_name = 'index.faiss'\n",
    "        if meta_file_name is None:\n",
    "            meta_file_name = 'index_meta.faiss'\n",
    "\n",
    "        index_file = os.path.join(dir_path, index_file_name)\n",
    "        meta_file = os.path.join(dir_path, meta_file_name)\n",
    "        print(f'Loading index from {index_file}, meta data from {meta_file}')\n",
    "\n",
    "        self.index = faiss.read_index(index_file)\n",
    "        print(f'Loaded index of type {type(self.index)} and size {self.index.ntotal}')\n",
    "\n",
    "        self.index_id_to_db_id = read_pickle(meta_file)\n",
    "        assert len(\n",
    "            self.index_id_to_db_id) == self.index.ntotal, 'Deserialized index_id_to_db_id should match faiss index size'\n",
    "        \n",
    "        # Move index to GPU if specified\n",
    "        if gpu_id is not None:\n",
    "            res = faiss.StandardGpuResources()  \n",
    "            self.index_gpu = faiss.index_cpu_to_gpu(res, gpu_id , self.index)\n",
    "            del self.index\n",
    "            self.index = self.index_gpu\n",
    "            print(f'Moved index to GPU {gpu_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_index() -> List[Indexer]:\n",
    "    \"\"\"Initialize and deserialize FAISS indexes.\"\"\"\n",
    "    indexes = []\n",
    "    if use_gpu:\n",
    "        for i, gpu_id in enumerate(gpu_ids):\n",
    "            index = Indexer(vector_size)\n",
    "            index.deserialize_from(\n",
    "                faiss_dir, \n",
    "                f'index{i+1}.faiss', f'index{i+1}_meta.faiss',\n",
    "                gpu_id=gpu_id\n",
    "            )\n",
    "            indexes.append(index)\n",
    "    else: # CPU\n",
    "        index = Indexer(vector_size)\n",
    "        index.deserialize_from(faiss_dir)\n",
    "        indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = initialize_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A wrapper class for encoding text using pre-trained transformer models with specified pooling strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: AutoConfig, pooling: str = \"average\"):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        if not hasattr(self.config, \"pooling\"):\n",
    "            self.config.pooling = pooling\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            config.name_or_path, config=self.config\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "    \n",
    "    def encode(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        normalize: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        model_output = self.forward(\n",
    "            input_ids, \n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "        )\n",
    "        last_hidden = model_output[\"last_hidden_state\"]\n",
    "        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.)\n",
    "\n",
    "        if self.config.pooling == \"average\":\n",
    "            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "        elif self.config.pooling == \"cls\":\n",
    "            emb = last_hidden[:, 0]\n",
    "\n",
    "        if normalize:\n",
    "            emb = F.normalize(emb, dim=-1)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \"\"\"\n",
    "    A class for retrieving document embeddings using a specified encoder, using a bi-encoder approach.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        query_encoder: Encoder,\n",
    "        doc_encoder: Optional[Encoder] = None,\n",
    "        max_length: int = 512,\n",
    "        add_special_tokens: bool = True,\n",
    "        norm_query_emb: bool = False,\n",
    "        norm_doc_emb: bool = False,\n",
    "        lower_case: bool = False,\n",
    "        do_normalize_text: bool = False,\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.query_encoder = query_encoder.to(device)\n",
    "        self.doc_encoder = self.query_encoder if doc_encoder is None else doc_encoder.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.norm_query_emb = norm_query_emb\n",
    "        self.norm_doc_emb = norm_doc_emb\n",
    "        self.lower_case = lower_case\n",
    "        self.do_normalize_text = do_normalize_text\n",
    "\n",
    "\n",
    "    def encode_queries(self, queries: List[str], batch_size: int) -> np.ndarray:\n",
    "        if self.do_normalize_text:\n",
    "            queries = [normalize_text.normalize(q) for q in queries]\n",
    "        if self.lower_case:\n",
    "            queries = [q.lower() for q in queries]\n",
    "\n",
    "        all_embeddings = []\n",
    "        nbatch = (len(queries) - 1) // batch_size + 1\n",
    "        with torch.no_grad():\n",
    "            for k in range(nbatch):\n",
    "                start_idx = k * batch_size\n",
    "                end_idx = min((k + 1) * batch_size, len(queries))\n",
    "\n",
    "                q_inputs = self.tokenizer(\n",
    "                    queries[start_idx:end_idx],\n",
    "                    max_length=self.max_length,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    add_special_tokens=self.add_special_tokens,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(self.device)\n",
    "\n",
    "                emb = self.query_encoder.encode(**q_inputs, normalize=self.norm_query_emb)\n",
    "                all_embeddings.append(emb.cpu())\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_retriever() -> Retriever:\n",
    "    \"\"\"Initialize the encoder and retriever.\"\"\"\n",
    "    config = AutoConfig.from_pretrained(\"facebook/contriever\")\n",
    "    encoder = Encoder(config).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n",
    "    retriever = Retriever(\n",
    "        device=device, tokenizer=tokenizer, \n",
    "        query_encoder=encoder, \n",
    "        max_length=max_length_encoder,\n",
    "        norm_query_emb=normalize_embeddings,\n",
    "        lower_case=lower_case,\n",
    "        do_normalize_text=normalize_text\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(retriever: Retriever, queries: List[str], batch_size: int) -> np.ndarray:\n",
    "    \"\"\"Encode queries using the retriever.\"\"\"\n",
    "    return retriever.encode_queries(queries, batch_size=batch_size).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = initialize_retriever()\n",
    "query_embeddings = process_queries(retriever, queries, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ip_search_results(\n",
    "    indexer1: Indexer, \n",
    "    indexer2: Indexer, \n",
    "    query_vectors: np.array, \n",
    "    top_docs: int, \n",
    "    index_batch_size: int = 2048\n",
    ") -> List[Tuple[List[str], List[float]]]:\n",
    "    \"\"\"\n",
    "    Merges the k-nearest neighbor search results from two different indices for a given set of query vectors.\n",
    "\n",
    "    Args:\n",
    "        indexer1 (Indexer): The first indexer object capable of performing knn searches.\n",
    "        indexer2 (Indexer): The second indexer object capable of performing knn searches.\n",
    "        query_vectors (np.array): A numpy array of query vectors for which to perform the searches.\n",
    "        top_docs (int): The number of top documents to retrieve from the combined results of the two indexer.\n",
    "        index_batch_size (int): The batch size to use for indexing operations.\n",
    "    \n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains two lists - the merged list of database IDs and the corresponding scores.\n",
    "    \"\"\"\n",
    "    # Perform searches on both indices\n",
    "    results1 = indexer1.search_knn(query_vectors, top_docs, index_batch_size)\n",
    "    results2 = indexer2.search_knn(query_vectors, top_docs, index_batch_size)\n",
    "\n",
    "    merged_results = []\n",
    "    for res1, res2 in zip(results1, results2):\n",
    "        # Merge the results from both indices\n",
    "        combined_db_ids = res1[0] + res2[0]\n",
    "        combined_scores = res1[1] + res2[1]\n",
    "\n",
    "        # Since we're using inner product, higher scores indicate better matches\n",
    "        # Combine and sort the results by score in descending order\n",
    "        combined = sorted(zip(combined_db_ids, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get only the top_docs results after merging\n",
    "        combined = combined[:top_docs]\n",
    "\n",
    "        # Separate the db_ids and scores again\n",
    "        db_ids, scores = zip(*combined)\n",
    "\n",
    "        merged_results.append((list(db_ids), list(scores)))\n",
    "\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(\n",
    "    indexes: List[Indexer], \n",
    "    query_embeddings: np.ndarray, \n",
    ") -> List[Tuple[List[str], List[float]]]:\n",
    "    \"\"\"Search documents using the indexes.\"\"\"\n",
    "    if use_gpu:\n",
    "        search_results = merge_ip_search_results(\n",
    "            indexes[0], indexes[1], query_embeddings, \n",
    "            top_docs=top_k, \n",
    "            index_batch_size=index_batch_size\n",
    "        )\n",
    "    else:\n",
    "        search_results = indexes[0].search_knn(\n",
    "            query_embeddings, top_docs=top_k, \n",
    "            index_batch_size=index_batch_size\n",
    "        )\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_results(\n",
    "    search_results: List[Tuple[List[str], List[float]]], \n",
    "):        \n",
    "    \"\"\"Save search results to a pickle file.\"\"\"\n",
    "    os.makedirs(search_dir, exist_ok=True)\n",
    "    file_path = os.path.join(\n",
    "        search_dir, f'{split}_search_results_at{top_k}.pkl'\n",
    "    )\n",
    "    write_pickle(search_results, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search_documents(indexes, query_embeddings)\n",
    "save_search_results(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
